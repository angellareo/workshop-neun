[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Computational Neuroscience with Neun",
    "section": "",
    "text": "Welcome to this hands-on workshop on computational neuroscience using the Neun library. This workshop is designed to provide you with both theoretical foundations and practical skills for modeling neural systems.\nFacilitators:\n\nDr.¬†√Ångel Lareo: angel.lareo@uam.es\nDr.¬†Alicia Garrido-Pe√±a: alicia.garrido@uam.es\n\nMore info about the facilitators‚Ä¶\nDuring the workshop, feel free to ask questions at any time.\nAfter the workshop, you can:\n\nOpen issues on the workshop repository\nContact the facilitators",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Introduction to Computational Neuroscience with Neun",
    "section": "",
    "text": "Welcome to this hands-on workshop on computational neuroscience using the Neun library. This workshop is designed to provide you with both theoretical foundations and practical skills for modeling neural systems.\nFacilitators:\n\nDr.¬†√Ångel Lareo: angel.lareo@uam.es\nDr.¬†Alicia Garrido-Pe√±a: alicia.garrido@uam.es\n\nMore info about the facilitators‚Ä¶\nDuring the workshop, feel free to ask questions at any time.\nAfter the workshop, you can:\n\nOpen issues on the workshop repository\nContact the facilitators",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#about-the-workshop",
    "href": "index.html#about-the-workshop",
    "title": "Introduction to Computational Neuroscience with Neun",
    "section": "About the Workshop",
    "text": "About the Workshop\nComputational neuroscience bridges the gap between experimental observations and theoretical understanding of neural systems. This workshop will guide you through:\n\nTheoretical Foundations: Understanding why and how to model neural systems\nPractical Skills: Learning to use the Neun library for neural simulations\nReal Applications: Create your own models and circuits!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#the-neun-library",
    "href": "index.html#the-neun-library",
    "title": "Introduction to Computational Neuroscience with Neun",
    "section": "The Neun Library",
    "text": "The Neun Library\nNeun is a C++ library for simulating neuronal networks. You can directly work in C++ or use our Python bindings to directly simulate your models in Python.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#workshop-structure",
    "href": "index.html#workshop-structure",
    "title": "Introduction to Computational Neuroscience with Neun",
    "section": "Workshop Structure",
    "text": "Workshop Structure\n\nPart 1: Introduction to Computational Neuroscience\nLearn the fundamental principles of neural modeling, from single neurons to networks. Understand the mathematical foundations and their biological relevance.\nDuration: ~20 min\nTopics:\n\nWhy model neural systems?\nSingle neuron dynamics\nFrom neurons to networks\n\n\n\nPart 2: Hands-on with Neun to simulate neural dynamics and networks\nGet practical experience building and simulating neural models using the Neun library.\nDuration: ~3 h\nTopics:\n\nInstalling and setting up Neun\nCreating single neuron models\nBuilding and simulating neural networks\nAdvanced modeling techniques\nAnalyzing the results\n\n\n\nPart 3: Conclusions and closing\nDuration: ~20 min\nTopics: - Analyzing neural dynamics",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Introduction to Computational Neuroscience with Neun",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo get the most out of this workshop, you should have:\n\nRequired: Basic Python programming skills\nRecommended: Experience with NumPy and Matplotlib\n\nIn your system, please ensure you have:\n\nPython 3.7 or higher, with development headers, installed on your computer\nC++ compiler with C++20 support (GCC 10+, Clang 10+)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#workshop-materials",
    "href": "index.html#workshop-materials",
    "title": "Introduction to Computational Neuroscience with Neun",
    "section": "Workshop Materials",
    "text": "Workshop Materials\nNavigate through the workshop using the menu above:\n\nPart 1: Computational Neuroscience - Theoretical foundations\nPart 2: Hands-on with Neun\n\nNeun Basics - Getting started with the library\nSingle Neurons - Modeling individual neurons\nNetworks - Building connected networks\n\nConclusions - Wrap-up",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "Introduction to Computational Neuroscience with Neun",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nNeun GitHub Repository\nNeun Python Bindings Documentation\n\n\nLet‚Äôs begin our journey into computational neuroscience! üß†",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "Introduction to Computational Neuroscience with Neun",
    "section": "Next Steps",
    "text": "Next Steps\nüëâ Start with Part 1: Introduction to Computational Neuroscience",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "facilitators.html",
    "href": "facilitators.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "facilitators.html#facilitators",
    "href": "facilitators.html#facilitators",
    "title": "",
    "section": "Facilitators",
    "text": "Facilitators\n\nDr.¬†Angel Lareo\nAssistant Professor at Universidad Aut√≥noma de Madrid. PhD in computational neuroscience. His work focuses on neural network modeling and he interdisciplinary applications of free technologies, exploring the potential of free/libre open source software and hardware in areas of social interest such as sustainability and neuroscience. In sustainability, he has developed low-power monitoring devices‚Äîsuch as the Arduino-based BtM datalogger‚Äîas well as a computational model of GHG emissions in eco-social transition scenarios. In the field of neuroscience, he began his research career working on closed-loop experimental studies, which led to his interest in the modeling and simulation of complex dynamic systems, developing and managing the Neun neural model library.\n\n\nDr.¬†Alicia Garrido-Pe√±a\nAssistant Professor at Universidad Aut√≥noma de Madrid. PhD in computational neuroscience. Her work focuses on Computational Neuroscience, trying to unveil neural dynamics from a hybrid and multidisciplinary approach: through electrophysiology experiments, computational models, and closed-loop real-time activity-dependent stimulation. As an early career researcher she is currently exploring other research paths in the field, such as the modeling of complex networks for cognitive neuroscience and the neurological problems caused by the addictive use of smartphones. Her other interests include, computer science, FLOSS and open and inclusive science. (https://agarpe.github.io/)"
  },
  {
    "objectID": "02-03-neural-networks.html",
    "href": "02-03-neural-networks.html",
    "title": "2.3: Synapses and Neural Networks",
    "section": "",
    "text": "Individual neurons are the building blocks, but the real computational power of the nervous system emerges from networks. In this section, we‚Äôll explore how to connect neurons through synapses and simulate network dynamics using Neun.\nUnderstanding networks requires grasping three key concepts:\n\nSynapses: The connections between neurons that enable communication\nConnectivity: The pattern of which neurons connect to which (the connectome)\nNetwork dynamics: The collective behavior that emerges from neuronal interactions",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Synapses & Networks"
    ]
  },
  {
    "objectID": "02-03-neural-networks.html#introduction",
    "href": "02-03-neural-networks.html#introduction",
    "title": "2.3: Synapses and Neural Networks",
    "section": "",
    "text": "Individual neurons are the building blocks, but the real computational power of the nervous system emerges from networks. In this section, we‚Äôll explore how to connect neurons through synapses and simulate network dynamics using Neun.\nUnderstanding networks requires grasping three key concepts:\n\nSynapses: The connections between neurons that enable communication\nConnectivity: The pattern of which neurons connect to which (the connectome)\nNetwork dynamics: The collective behavior that emerges from neuronal interactions",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Synapses & Networks"
    ]
  },
  {
    "objectID": "02-03-neural-networks.html#types-of-synapses",
    "href": "02-03-neural-networks.html#types-of-synapses",
    "title": "2.3: Synapses and Neural Networks",
    "section": "Types of Synapses",
    "text": "Types of Synapses\nNeun provides two main types of synaptic connections, each with distinct properties and biological relevance:\n\nElectrical SynapsesDiffusion Synapses\n\n\nElectrical synapses (gap junctions) provide direct electrical coupling between neurons through protein channels that connect their cytoplasm.\nThe current through a gap junction is proportional to the voltage difference: \\[I_{syn} = g_{gap} \\cdot (V_1 - V_2)\\]\nwhere \\(g_{gap}\\) is the gap junction conductance (typically 0.001-0.01).\nThis creates:\n\nInstantaneous transmission: No synaptic delay\nBidirectional communication: Current flows both ways\nSynchronization: Tendency to align neuron activity\n\nStructure: ESynN1N2PrecisionIntegrator\n\n\nsrc/electrical-synapse.py\n\n#!/usr/bin/env python3\n\"\"\"\nTwo neurons coupled via electrical synapse (gap junction)\nDemonstrates instantaneous bidirectional coupling\n\"\"\"\nimport neun_py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create two Hodgkin-Huxley neurons\nneuron_args = neun_py.HHDoubleConstructorArgs()\nh1 = neun_py.HHDoubleRK4(neuron_args)\nh2 = neun_py.HHDoubleRK4(neuron_args)\n\n# Set parameters for both neurons\nfor neuron in [h1, h2]:\n    neuron.set_param(neun_py.HHDoubleParameter.cm, 1 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.vna, 50)\n    neuron.set_param(neun_py.HHDoubleParameter.vk, -77)\n    neuron.set_param(neun_py.HHDoubleParameter.vl, -54.387)\n    neuron.set_param(neun_py.HHDoubleParameter.gna, 120 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.gk, 36 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.gl, 0.3 * 7.854e-3)\n\n# Set different initial voltages\nh1.set(neun_py.HHDoubleVariable.v, -75)\nh2.set(neun_py.HHDoubleVariable.v, -65)\n\n# Create electrical synapse with conductance\n# Negative conductance values create the proper coupling\nsynapse = neun_py.ESynHHHHDoubleRK4(\n    h1, neun_py.HHDoubleVariable.v,\n    h2, neun_py.HHDoubleVariable.v,\n    0.002,  # Conductance from h1 to h2\n    0.002  # Conductance from h2 to h1\n)\n\n# Simulation parameters\nstep = 0.001  # ms\nduration = 100  # ms\n\n# Storage\ntimes = []\nv1_values = []\nv2_values = []\nsynaptic_currents = []\n\n# Run simulation\ntime = 0.0\nwhile time &lt; duration:\n    # Step the synapse first (updates coupling)\n    synapse.step(step)\n    \n    # Add external input only to first neuron\n    h1.add_synaptic_input(0.1)\n    \n    # Step both neurons\n    h1.step(step)\n    h2.step(step)\n    \n    # Record data\n    times.append(time)\n    v1_values.append(h1.get(neun_py.HHDoubleVariable.v))\n    v2_values.append(h2.get(neun_py.HHDoubleVariable.v))\n    \n    # Get synaptic current\n    i_syn = synapse.get(neun_py.ESynDoubleVariable.i1)\n    synaptic_currents.append(i_syn)\n    \n    time += step\n\n# Plot results\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n\n# Membrane potentials\nax1.plot(times, v1_values, 'b-', label='Neuron 1 (with input)', linewidth=1.5)\nax1.plot(times, v2_values, 'r-', label='Neuron 2 (coupled)', linewidth=1.5)\nax1.set_ylabel('Membrane Potential (mV)')\nax1.set_title('Electrical Synapse: Bidirectional Coupling via Gap Junction')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Synaptic current\nax2.plot(times, synaptic_currents, 'g-', linewidth=1.5)\nax2.set_xlabel('Time (ms)')\nax2.set_ylabel('Synaptic Current')\nax2.set_title('Coupling Current Through Gap Junction')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('electrical_synapse.pdf')\nplt.show()\n\n\n\n\n\n\n\nTipHands-on\n\n\n\nTry modifying the coupling conductance‚Ä¶ Are you able to change the synchronization level? üíª\n\n\nSolution\n\n\nChange 0.002 to 0.001 (weaker coupling)\nChange to 0.003 (stronger coupling)\n\nYou can tell the strength of the coupling from the distance between peaks.\n\n\n\n\n\nDiffusion synapses model chemical-like coupling with time-dependent dynamics that approximate neurotransmitter diffusion across the synaptic cleft:\n\nSlower transmission: Time-dependent dynamics\nFiltering effects: Can smooth rapid fluctuations\n\nBiological realism: Better models chemical synapses\n\nStructure: DSynN1N2PrecisionIntegrator\n\n\nsrc/diffusion-synapse.py\n\n#!/usr/bin/env python3\n\"\"\"\nTwo neurons coupled via diffusion synapse\nDemonstrates chemical-like coupling with dynamics\n\"\"\"\nimport neun_py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create two Hodgkin-Huxley neurons\nneuron_args = neun_py.HHDoubleConstructorArgs()\nh1 = neun_py.HHDoubleRK4(neuron_args)\nh2 = neun_py.HHDoubleRK4(neuron_args)\n\n# Set parameters for both neurons\nfor neuron in [h1, h2]:\n    neuron.set_param(neun_py.HHDoubleParameter.cm, 1 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.vna, 50)\n    neuron.set_param(neun_py.HHDoubleParameter.vk, -77)\n    neuron.set_param(neun_py.HHDoubleParameter.vl, -54.387)\n    neuron.set_param(neun_py.HHDoubleParameter.gna, 120 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.gk, 36 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.gl, 0.3 * 7.854e-3)\n\n# Set different initial voltages\nh1.set(neun_py.HHDoubleVariable.v, -70)\nh2.set(neun_py.HHDoubleVariable.v, -65)\n\n# Create diffusion synapse (chemical-like dynamics)\nsynapse = neun_py.DSynHHHHDoubleRK4(\n    h1, neun_py.HHDoubleVariable.v,\n    h2, neun_py.HHDoubleVariable.v\n)\n\n# Simulation parameters\nstep = 0.001  # ms\nduration = 100  # ms\n\n# Storage\ntimes = []\nv1_values = []\nv2_values = []\n\n# Run simulation\ntime = 0.0\nwhile time &lt; duration:\n    # Step the synapse first\n    synapse.step(step)\n    \n    # Add external input to first neuron\n    h1.add_synaptic_input(0.12)\n    \n    # Step both neurons\n    h1.step(step)\n    h2.step(step)\n    \n    # Record data\n    times.append(time)\n    v1_values.append(h1.get(neun_py.HHDoubleVariable.v))\n    v2_values.append(h2.get(neun_py.HHDoubleVariable.v))\n    \n    time += step\n\n# Plot results\nplt.figure(figsize=(10, 6))\nplt.plot(times, v1_values, 'b-', label='Neuron 1 (presynaptic)', linewidth=1.5)\nplt.plot(times, v2_values, 'r-', label='Neuron 2 (postsynaptic)', linewidth=1.5)\nplt.xlabel('Time (ms)')\nplt.ylabel('Membrane Potential (mV)')\nplt.title('Diffusion Synapse: Chemical-like Coupling with Dynamics')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('diffusion_synapse.pdf')\nplt.show()\n\n\n\n\n\n\n\n\nTipHands-on\n\n\n\nCompare the two synapse types‚Ä¶ üíª\nRun both examples and observe: - Which shows faster coupling? - Which produces tighter synchronization? - How do the voltage traces differ?\n\n\n\n\n\n\n\n\n\n\n\nImportantRemember: Synapse Naming Convention\n\n\n\nNeun follows a systematic naming pattern for synapses:\n{SynapseType}{Neuron1Type}{Neuron2Type}{Precision}{Integrator}\nExamples: - ESynHHHHDoubleRK4 - Electrical synapse between two HH neurons - DSynHRHRDoubleRK6 - Diffusion synapse between two HR neurons\n- ESynHHHRFloatRK4 - Electrical synapse from HH to HR neuron\nImportant: Always step synapses before stepping neurons in your simulation loop!",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Synapses & Networks"
    ]
  },
  {
    "objectID": "02-03-neural-networks.html#building-neural-networks",
    "href": "02-03-neural-networks.html#building-neural-networks",
    "title": "2.3: Synapses and Neural Networks",
    "section": "Building Neural Networks",
    "text": "Building Neural Networks\n\nFeedforward NetworkRaster Plots\n\n\nLet‚Äôs build a simple feedforward chain where activity propagates from one neuron to the next:\n\n\nsrc/feedforward.py\n\n#!/usr/bin/env python3\n\"\"\"\nChain of neurons connected via electrical synapses\nDemonstrates signal propagation through a network\n\"\"\"\nimport neun_py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_hh_neuron(v_init=-65):\n    \"\"\"Helper function to create and initialize an HH neuron\"\"\"\n    neuron_args = neun_py.HHDoubleConstructorArgs()\n    neuron = neun_py.HHDoubleRK4(neuron_args)\n    \n    # Set parameters\n    neuron.set_param(neun_py.HHDoubleParameter.cm, 1 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.vna, 50)\n    neuron.set_param(neun_py.HHDoubleParameter.vk, -77)\n    neuron.set_param(neun_py.HHDoubleParameter.vl, -54.387)\n    neuron.set_param(neun_py.HHDoubleParameter.gna, 120 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.gk, 36 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.gl, 0.3 * 7.854e-3)\n    \n    # Set initial voltage\n    neuron.set(neun_py.HHDoubleVariable.v, v_init)\n    neuron.set(neun_py.HHDoubleVariable.m, 0.05)\n    neuron.set(neun_py.HHDoubleVariable.h, 0.6)\n    neuron.set(neun_py.HHDoubleVariable.n, 0.3)\n    \n    return neuron\n\n# Create a chain of 4 neurons with different initial conditions\nneurons = [\n    create_hh_neuron(-70),\n    create_hh_neuron(-68),\n    create_hh_neuron(-66),\n    create_hh_neuron(-64)\n]\n\n# Connect them in a chain with electrical synapses\nsynapses = []\nfor i in range(len(neurons) - 1):\n    synapse = neun_py.ESynHHHHDoubleRK4(\n        neurons[i], neun_py.HHDoubleVariable.v,\n        neurons[i+1], neun_py.HHDoubleVariable.v,\n        0.002, 0.002  # Bidirectional coupling\n    )\n    synapses.append(synapse)\n\n# Simulation parameters\nstep = 0.001  # ms\nduration = 150  # ms\n\n# Storage\ntimes = []\nvoltages = [[] for _ in neurons]\n\n# Run simulation\ntime = 0.0\nwhile time &lt; duration:\n    # Step all synapses first\n    for synapse in synapses:\n        synapse.step(step)\n    \n    # Add input only to first neuron\n    neurons[0].add_synaptic_input(0.15)\n    \n    # Step all neurons\n    for neuron in neurons:\n        neuron.step(step)\n    \n    # Record data\n    times.append(time)\n    for i, neuron in enumerate(neurons):\n        voltages[i].append(neuron.get(neun_py.HHDoubleVariable.v))\n    \n    time += step\n\n# Plot results\nplt.figure(figsize=(12, 6))\ncolors = ['blue', 'red', 'green', 'purple']\nfor i, (v, color) in enumerate(zip(voltages, colors)):\n    plt.plot(times, v, color=color, label=f'Neuron {i+1}', \n             linewidth=1.5, alpha=0.8)\n\nplt.xlabel('Time (ms)')\nplt.ylabel('Membrane Potential (mV)')\nplt.title('Signal Propagation Through a Chain of Coupled Neurons')\nplt.legend(loc='upper right')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('neuron_chain.pdf')\nplt.show()\n\nObservation: Notice how the spike travels through the chain with some delay. The coupling strength determines how effectively the signal propagates.\n\n\n\n\n\n\nTipHands-on\n\n\n\nExperiment with the chain‚Ä¶ üíª\n\nChange the coupling strength (currently -0.002)\nAdd input to the last neuron instead of the first\nAdd more neurons to the chain (change 4 to 6 or 8)\n\nWhat‚Äôs the maximum chain length before signal dies out?\n\n\n\n\nRaster plots show spike times for all neurons, revealing temporal patterns:\n\n\nsrc/raster-plot.py\n\n#!/usr/bin/env python3\n\"\"\"\nNetwork activity visualization with raster plot\nShows spike times for multiple neurons\n\"\"\"\nimport neun_py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_hh_neuron(v_init=-65):\n    \"\"\"Create and initialize an HH neuron\"\"\"\n    neuron_args = neun_py.HHDoubleConstructorArgs()\n    neuron = neun_py.HHDoubleRK4(neuron_args)\n    \n    neuron.set_param(neun_py.HHDoubleParameter.cm, 1 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.vna, 50)\n    neuron.set_param(neun_py.HHDoubleParameter.vk, -77)\n    neuron.set_param(neun_py.HHDoubleParameter.vl, -54.387)\n    neuron.set_param(neun_py.HHDoubleParameter.gna, 120 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.gk, 36 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.gl, 0.3 * 7.854e-3)\n    \n    neuron.set(neun_py.HHDoubleVariable.v, v_init)\n    neuron.set(neun_py.HHDoubleVariable.m, 0.05)\n    neuron.set(neun_py.HHDoubleVariable.h, 0.6)\n    neuron.set(neun_py.HHDoubleVariable.n, 0.3)\n    \n    return neuron\n\n# Create 5 neurons\nn_neurons = 5\nneurons = [create_hh_neuron(-65 + i*2) for i in range(n_neurons)]\n\n# Connect in a simple network (each to next)\nsynapses = []\nfor i in range(n_neurons - 1):\n    synapse = neun_py.ESynHHHHDoubleRK4(\n        neurons[i], neun_py.HHDoubleVariable.v,\n        neurons[i+1], neun_py.HHDoubleVariable.v,\n        -0.001, -0.001\n    )\n    synapses.append(synapse)\n\n# Simulation parameters\nstep = 0.001  # ms\nduration = 200  # ms\n\n# Storage for spike detection\nspike_threshold = 0  # mV\nspike_times = [[] for _ in range(n_neurons)]\nwas_below_threshold = [True] * n_neurons\n\ntimes = []\nvoltages = [[] for _ in range(n_neurons)]\n\n# Run simulation\ntime = 0.0\nwhile time &lt; duration:\n    # Step synapses\n    for synapse in synapses:\n        synapse.step(step)\n    \n    # Add different inputs to create varied activity\n    for i, neuron in enumerate(neurons):\n        # First neuron gets constant input, others get less\n        input_current = 0.12 if i == 0 else 0.08 if i == 1 else 0.0\n        neuron.add_synaptic_input(input_current)\n    \n    # Step neurons\n    for neuron in neurons:\n        neuron.step(step)\n    \n    # Record and detect spikes\n    times.append(time)\n    for i, neuron in enumerate(neurons):\n        v = neuron.get(neun_py.HHDoubleVariable.v)\n        voltages[i].append(v)\n        \n        # Spike detection: crossing threshold from below\n        if was_below_threshold[i] and v &gt; spike_threshold:\n            spike_times[i].append(time)\n            was_below_threshold[i] = False\n        elif v &lt; spike_threshold:\n            was_below_threshold[i] = True\n    \n    time += step\n\n# Create figure with two subplots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n\n# Raster plot\nfor i, spikes in enumerate(spike_times):\n    ax1.scatter(spikes, [i] * len(spikes), c='black', marker='|', s=100)\n\nax1.set_ylabel('Neuron ID')\nax1.set_xlabel('Time (ms)')\nax1.set_title('Network Activity: Raster Plot')\nax1.set_ylim(-0.5, n_neurons - 0.5)\nax1.set_yticks(range(n_neurons))\nax1.grid(True, alpha=0.3)\n\n# Voltage traces (subset for clarity)\ncolors = plt.cm.viridis(np.linspace(0, 1, n_neurons))\nfor i in range(n_neurons):\n    ax2.plot(times, voltages[i], color=colors[i], \n             label=f'Neuron {i}', linewidth=1.0, alpha=0.7)\n\nax2.set_xlabel('Time (ms)')\nax2.set_ylabel('Membrane Potential (mV)')\nax2.set_title('Network Activity: Voltage Traces')\nax2.legend(loc='upper right', ncol=n_neurons)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('raster_plot.pdf')\nplt.show()\n\n# Print spike statistics\nprint(\"Spike Statistics:\")\nfor i, spikes in enumerate(spike_times):\n    print(f\"  Neuron {i}: {len(spikes)} spikes\")\n    if len(spikes) &gt; 1:\n        isis = np.diff(spikes)\n        print(f\"    Mean ISI: {np.mean(isis):.2f} ms\")\n        print(f\"    Firing rate: {len(spikes)/(duration/1000):.2f} Hz\")\n\nWhat to look for: - Synchrony: Vertical alignment of spikes - Propagation: Diagonal patterns - Rhythms: Regular spacing\n\n\n\n\n\n\nTipHands-on\n\n\n\nAnalyze the raster plot‚Ä¶ üíª\n\nCan you identify synchronized events?\nDo you see traveling waves?\nTry increasing the coupling strength - what changes?",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Synapses & Networks"
    ]
  },
  {
    "objectID": "02-03-neural-networks.html#central-pattern-generators-cpgs",
    "href": "02-03-neural-networks.html#central-pattern-generators-cpgs",
    "title": "2.3: Synapses and Neural Networks",
    "section": "Central Pattern Generators (CPGs)",
    "text": "Central Pattern Generators (CPGs)\nCPGs are neural circuits that produce rhythmic patterns without rhythmic input. They‚Äôre essential for locomotion, breathing, and other rhythmic behaviors.\n\n\n\nA figure of CPG dynamics\n\n\nKey features of CPGs:\n\nIntrinsic rhythmicity: Generated by network dynamics, not input\nPhase relationships: Neurons fire in specific temporal order\nRobustness: Maintain rhythm despite perturbations\n\n\nReciprocal Inhibition Model\nA classic CPG uses two neurons with mutual inhibition creating alternating bursts:\n\n\nsrc/cpg-analysis.py\n\n#!/usr/bin/env python3\n\"\"\"\nCentral Pattern Generator (CPG) Analysis\nDemonstrates rhythmic activity and phase relationships\n\"\"\"\nimport neun_py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_hr_neuron(I_ext=3.0, ini_x=0):\n    \"\"\"Create Hindmarsh-Rose neuron (good for rhythmic activity)\"\"\"\n    neuron_args = neun_py.HRDoubleConstructorArgs()\n    neuron = neun_py.HRDoubleRK4(neuron_args)\n    \n    # HR parameters for bursting\n    neuron.set_param(neun_py.HRDoubleParameter.e, I_ext)\n    neuron.set_param(neun_py.HRDoubleParameter.a, 1.0)\n    neuron.set_param(neun_py.HRDoubleParameter.b, 3.0)\n    neuron.set_param(neun_py.HRDoubleParameter.c, 1.0)\n    neuron.set_param(neun_py.HRDoubleParameter.d, 5.0)\n    neuron.set_param(neun_py.HRDoubleParameter.mu, 0.006)\n    neuron.set_param(neun_py.HRDoubleParameter.S, 4.0)\n    neuron.set_param(neun_py.HRDoubleParameter.xr, -1.6)\n    neuron.set_param(neun_py.HRDoubleParameter.vh, 1)\n    \n    # Initial conditions\n    neuron.set(neun_py.HRDoubleVariable.x, ini_x)\n    neuron.set(neun_py.HRDoubleVariable.y, -10.0)\n    neuron.set(neun_py.HRDoubleVariable.z, 0.0)\n    \n    return neuron\n\n# Create two HR neurons with reciprocal inhibition (CPG configuration)\nn1 = create_hr_neuron(3.0, -1.5)\nn2 = create_hr_neuron(3.0, 0)\n\n\n# Reciprocal coupling (simulating mutual inhibition)\ns12 = neun_py.ESynHRHRDoubleRK4(\n    n1, neun_py.HRDoubleVariable.x,\n    n2, neun_py.HRDoubleVariable.x,\n    -0.75, -0.75  # Inhibitory-like coupling\n)\n\n# Simulation parameters\nstep = 0.01  # ms\nduration = 1000  # ms (longer to see rhythm)\n\n# Storage\ntimes = []\nx1_values = []\nx2_values = []\n\n# Run simulation\ntime = 0.0\nwhile time &lt; duration:\n    s12.step(step)\n    n1.step(step)\n    n2.step(step)\n    \n    times.append(time)\n    x1_values.append(n1.get(neun_py.HRDoubleVariable.x))\n    x2_values.append(n2.get(neun_py.HRDoubleVariable.x))\n    \n    time += step\n\n# Convert to numpy arrays\ntimes = np.array(times)\nx1_values = np.array(x1_values)\nx2_values = np.array(x2_values)\n\nplt.plot(times, x1_values, 'b-', label='Neuron 1')\nplt.plot(times, x2_values, 'r-', label='Neuron 2')\nplt.legend()\nplt.show()\n# Detect burst peaks (local maxima above threshold)\ndef detect_bursts(signal, threshold=0):\n    bursts = []\n    for i in range(1, len(signal)-1):\n        if signal[i] &gt; threshold and signal[i] &gt; signal[i-1] and signal[i] &gt; signal[i+1]:\n            bursts.append(i)\n    return np.array(bursts)\n\nbursts1 = detect_bursts(x1_values, threshold=1.0)\nbursts2 = detect_bursts(x2_values, threshold=1.0)\n\n# Compute inter-burst intervals (IBIs)\ndef compute_ibis(burst_indices, times):\n    if len(burst_indices) &lt; 2:\n        return np.array([])\n    burst_times = times[burst_indices]\n    return np.diff(burst_times)\n\nibi1 = compute_ibis(bursts1, times)\nibi2 = compute_ibis(bursts2, times)\n\n# Compute phase difference (time lag between bursts)\ndef compute_phase_lag(bursts1, bursts2, times):\n    if len(bursts1) == 0 or len(bursts2) == 0:\n        return []\n    \n    phase_lags = []\n    for b1 in bursts1:\n        # Find nearest burst in neuron 2\n        if len(bursts2) &gt; 0:\n            nearest = bursts2[np.argmin(np.abs(bursts2 - b1))]\n            lag = times[nearest] - times[b1]\n            phase_lags.append(lag)\n    \n    return np.array(phase_lags)\n\nphase_lags = compute_phase_lag(bursts1, bursts2, times)\n\n# Visualization\nfig = plt.figure(figsize=(14, 10))\n\n# Activity traces\nax1 = plt.subplot(3, 2, (1, 2))\nax1.plot(times, x1_values, 'b-', linewidth=1.5, label='Neuron 1', alpha=0.8)\nax1.plot(times, x2_values, 'r-', linewidth=1.5, label='Neuron 2', alpha=0.8)\nax1.plot(times[bursts1], x1_values[bursts1], 'bo', markersize=4)\nax1.plot(times[bursts2], x2_values[bursts2], 'ro', markersize=4)\nax1.set_xlabel('Time (ms)')\nax1.set_ylabel('Membrane Potential')\nax1.set_title('Central Pattern Generator: Rhythmic Activity')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Raster plot\nax2 = plt.subplot(3, 2, 3)\nax2.scatter(times[bursts1], np.ones_like(bursts1), c='blue', marker='|', s=100)\nax2.scatter(times[bursts2], np.zeros_like(bursts2), c='red', marker='|', s=100)\nax2.set_yticks([0, 1])\nax2.set_yticklabels(['Neuron 2', 'Neuron 1'])\nax2.set_xlabel('Time (ms)')\nax2.set_title('Burst Timing (Raster)')\nax2.grid(True, alpha=0.3)\n\n# Inter-burst intervals\nax3 = plt.subplot(3, 2, 4)\nif len(ibi1) &gt; 0:\n    ax3.plot(ibi1, 'bo-', label='Neuron 1', markersize=4)\nif len(ibi2) &gt; 0:\n    ax3.plot(ibi2, 'ro-', label='Neuron 2', markersize=4)\nax3.set_xlabel('Burst Number')\nax3.set_ylabel('Inter-Burst Interval (ms)')\nax3.set_title('Rhythm Regularity')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Phase relationship\nax4 = plt.subplot(3, 2, 5)\nif len(phase_lags) &gt; 0:\n    ax4.hist(phase_lags, bins=20, color='purple', alpha=0.7, edgecolor='black')\n    ax4.axvline(np.mean(phase_lags), color='red', linestyle='--', \n                linewidth=2, label=f'Mean: {np.mean(phase_lags):.1f} ms')\nax4.set_xlabel('Phase Lag (ms)')\nax4.set_ylabel('Count')\nax4.set_title('Phase Relationship Between Neurons')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\n# Zoomed view\nax5 = plt.subplot(3, 2, 6)\nzoom_start = int(len(times) * 0.3)\nzoom_end = int(len(times) * 0.5)\nax5.plot(times[zoom_start:zoom_end], x1_values[zoom_start:zoom_end], \n         'b-', linewidth=2, label='Neuron 1')\nax5.plot(times[zoom_start:zoom_end], x2_values[zoom_start:zoom_end], \n         'r-', linewidth=2, label='Neuron 2')\nax5.set_xlabel('Time (ms)')\nax5.set_ylabel('Membrane Potential')\nax5.set_title('Zoomed View: Alternating Bursts')\nax5.legend()\nax5.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('cpg_analysis.pdf')\nplt.show()\n\n# Print statistics\nprint(\"CPG Statistics:\")\nprint(f\"  Neuron 1: {len(bursts1)} bursts\")\nprint(f\"  Neuron 2: {len(bursts2)} bursts\")\nif len(ibi1) &gt; 0:\n    print(f\"  Mean IBI Neuron 1: {np.mean(ibi1):.2f} ¬± {np.std(ibi1):.2f} ms\")\nif len(ibi2) &gt; 0:\n    print(f\"  Mean IBI Neuron 2: {np.mean(ibi2):.2f} ¬± {np.std(ibi2):.2f} ms\")\nif len(phase_lags) &gt; 0:\n    print(f\"  Mean phase lag: {np.mean(phase_lags):.2f} ¬± {np.std(phase_lags):.2f} ms\")\n\n\n\n\n\n\n\nNoteCPG Analysis Metrics\n\n\n\nInter-Burst Interval (IBI): Time between successive bursts in one neuron - Measures rhythm regularity - CV of IBI &lt; 0.1 indicates regular rhythm\nBurst duration: Time from first to last spike in a burst - Measures rhythm regularity - Characterizes a phase in the cycle\n\n\n\n\n\n\n\n\nTipHands-on\n\n\n\nModify the CPG‚Ä¶ üíª\n\nChange coupling strength (-0.05 ‚Üí -0.03 or -0.08)\nTry asymmetric coupling (different values for each direction). How does this affect coordination?\nAdjust external drive I parameter\n\nCan you create a different phase relationship?\n\n\nHint\n\n2: Use different values for the two conductance parameters:\nsynapse = neun_py.ESynHHHHDoubleRK4(h1, v, h2, v, -0.003, -0.001)",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Synapses & Networks"
    ]
  },
  {
    "objectID": "02-03-neural-networks.html#excitation-inhibition-balance",
    "href": "02-03-neural-networks.html#excitation-inhibition-balance",
    "title": "2.3: Synapses and Neural Networks",
    "section": "Excitation-Inhibition Balance",
    "text": "Excitation-Inhibition Balance\nReal neural networks contain both excitatory and inhibitory neurons. The balance between them is critical for stable dynamics.\n\n\n\n\n\n\nNoteE-I Balance Principles\n\n\n\n\nToo much excitation: Runaway activity, seizures\nToo much inhibition: Network shuts down, no response\n\nBalanced: Stable, responsive, rich dynamics\n\nTypical cortical networks: ~80% excitatory, ~20% inhibitory neurons\nCurrent Neun implementation focuses on electrical and diffusion coupling. For explicit excitatory/inhibitory synapses, you can: 1. Use coupling conductance sign and magnitude 2. Implement through the external input mechanism 3. Wait for future Neun updates with chemical synapse models!",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Synapses & Networks"
    ]
  },
  {
    "objectID": "02-03-neural-networks.html#best-practices-for-network-simulations",
    "href": "02-03-neural-networks.html#best-practices-for-network-simulations",
    "title": "2.3: Synapses and Neural Networks",
    "section": "Best Practices for Network Simulations",
    "text": "Best Practices for Network Simulations\n\n\n\n\n\n\nImportantSimulation Tips\n\n\n\n1. Update Order Matters\n# Correct order:\nfor synapse in synapses:\n    synapse.step(step)\n    \nfor neuron in neurons:\n    neuron.step(step)\n2. Pre-allocate Storage\n# Efficient for large networks\nn_steps = int(duration / step)\nvoltages = np.zeros((n_neurons, n_steps))\n3. Choose Appropriate Time Steps - Too large: Numerical instability - Too small: Slow simulation - Typical: 0.001-0.01 ms for HH neurons\n4. Monitor for Pathological States - Check for NaN or infinite values - Watch for runaway excitation - Validate with known behaviors\n\n\nKey takeaways:\n\nSynapses create the connectivity that enables network computation\nSmall changes in coupling can dramatically alter network behavior\nAppropriate visualization reveals patterns invisible in raw data\nCPGs demonstrate how networks generate complex behaviors autonomously",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Synapses & Networks"
    ]
  },
  {
    "objectID": "02-03-neural-networks.html#exercises",
    "href": "02-03-neural-networks.html#exercises",
    "title": "2.3: Synapses and Neural Networks",
    "section": "Exercises",
    "text": "Exercises\nTest your understanding with these challenges:\n\nCritical coupling: Find the minimum coupling strength needed for a chain of 5 neurons to propagate a signal from first to last.\nNetwork motifs: Build an all-to-all neural network. Check its dynamics using a raster plot.\nHeterogeneous network: Create a network mixing HH and HR neurons. Use ESynHHHRDoubleRK4 for cross-type connections.\nOscillation frequency: Modify CPG parameters to generate faster rhythms. Can you reach 10 Hz? 50 Hz?\nInterval analysis: For different intervals in the CPG (within the same neuron or between neurons), compute the difference in duration of the interval between cycles as a percentage of each cycle period.\n\n\n\nSolution Hints\n\nEx 3: Mix neuron types by creating different neurons and using cross-type synapse classes:\nhh_neuron = neun_py.HHDoubleRK4(hh_args)\nhr_neuron = neun_py.HRDoubleRK4(hr_args)\nsynapse = neun_py.ESynHHHRDoubleRK4(hh_neuron, hh_v, hr_neuron, hr_v, -g, -g)\nEx 6: Increase external drive I parameter in HR neurons for faster bursting.\n\n\nüëâ Continue to Part 3: Conclusions",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Synapses & Networks"
    ]
  },
  {
    "objectID": "02-02-single-neurons.html",
    "href": "02-02-single-neurons.html",
    "title": "2.2: Modeling Neurons",
    "section": "",
    "text": "In this section, we‚Äôll explore single neuron models in depth, understanding their properties, behaviors, and how to use them effectively with Neun. While single neurons may seem simple, they exhibit rich dynamics that are essential for understanding network behavior.",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Neurons"
    ]
  },
  {
    "objectID": "02-02-single-neurons.html#introduction",
    "href": "02-02-single-neurons.html#introduction",
    "title": "2.2: Modeling Neurons",
    "section": "",
    "text": "In this section, we‚Äôll explore single neuron models in depth, understanding their properties, behaviors, and how to use them effectively with Neun. While single neurons may seem simple, they exhibit rich dynamics that are essential for understanding network behavior.",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Neurons"
    ]
  },
  {
    "objectID": "02-02-single-neurons.html#exploring-neuron-models",
    "href": "02-02-single-neurons.html#exploring-neuron-models",
    "title": "2.2: Modeling Neurons",
    "section": "Exploring Neuron Models",
    "text": "Exploring Neuron Models\n\nHodgkin-Huxley (HH)Hindmarsh-Rose (HR)Izhikevich (Iz)\n\n\nThe classic biophysical neuron model with sodium and potassium conductances:\n\n\n\nHodgkin-Huxley formalism equations\n\n\n\n\nsrc/hh.py\n\nimport neun_py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Compare different external currents\ncurrents = [0.05, 0.1, 0.15]\ncolors = ['blue', 'green', 'red']\n\n# Create subplots for each current\nfig, axes = plt.subplots(len(currents), 1, figsize=(10, 8), sharex=True)\n\ndt = 0.001           # Time step (ms)\nT = 100              # Simulation duration (ms)\ntime = np.arange(0, T, dt)\n\nfor idx, (I_ext, color) in enumerate(zip(currents, colors)):\n    # Create neuron\n    args = neun_py.HHDoubleConstructorArgs()\n    neuron = neun_py.HHDoubleRK4(args)\n    \n    # Set parameters\n    neuron.set_param(neun_py.HHDoubleParameter.cm, 1.0 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.vna, 50.0)\n    neuron.set_param(neun_py.HHDoubleParameter.vk, -77.0)\n    neuron.set_param(neun_py.HHDoubleParameter.vl, -54.387)\n    neuron.set_param(neun_py.HHDoubleParameter.gna, 120 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.gk, 36 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.gl, 0.3 * 7.854e-3)\n    \n    # Set initial conditions\n    neuron.set(neun_py.HHDoubleVariable.v, -80.0)\n    neuron.set(neun_py.HHDoubleVariable.m, 0.1)\n    neuron.set(neun_py.HHDoubleVariable.n, 0.7)\n    neuron.set(neun_py.HHDoubleVariable.h, 0.01)\n\n    # Simulate\n    V = []\n    for t in time:\n        neuron.add_synaptic_input(I_ext)\n        neuron.step(dt)\n        V.append(neuron.get(neun_py.HHDoubleVariable.v))\n    \n    # Plot\n    axes[idx].plot(time, V, color=color, linewidth=1.5)\n    axes[idx].set_ylabel('V (mV)')\n    axes[idx].set_title(f'External Current = {I_ext} nA')\n    axes[idx].grid(True, alpha=0.3)\n\naxes[-1].set_xlabel('Time (ms)')\nplt.suptitle('Effect of Input Current on Hodgkin-Huxley Neuron', y=1.02)\nplt.tight_layout()\nplt.show()\n\nObservation: Higher external current leads to: - More frequent action potentials - Higher firing rate - Different spiking patterns\n\n\n\n\n\n\nTipHands-on\n\n\n\nTry different current values‚Ä¶ üíª\nCan you find the model‚Äôs limit? Do you think a living neuron would have a similar limit?\n\n\n\n\nA reduced model that can produce bursting and chaotic behavior:\n\\[\\begin{cases}\n\\dot x = y - a x^{3} + b x^{2} + I - z \\\\\n\\dot y = c - d x^{2} - y \\\\\n\\dot z = r\\big(s(x - x_R) - z\\big)\n\\end{cases}\\]\n\n\nsrc/hr.py\n\nimport neun_py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Different parameter sets for HR neuron\nparameter_sets = [\n    {'name': 'Regular Spiking', 'I': 3.8},\n    {'name': 'Bursting', 'I': 2.5},\n    {'name': 'Chaotic', 'I': 3.2},\n]\n\nfig, axes = plt.subplots(len(parameter_sets), 1, figsize=(12, 8))\n\ndt = 0.01\nT = 5000\ntime = np.arange(0, T, dt)\n\nfor idx, params in enumerate(parameter_sets):\n    # Create Hindmarsh-Rose neuron\n    args = neun_py.HRDoubleConstructorArgs()\n    neuron = neun_py.HRDoubleRK4(args)\n    \n    # Set parameters\n    neuron.set_param(neun_py.HRDoubleParameter.e, 0)\n    neuron.set_param(neun_py.HRDoubleParameter.mu, 0.006)\n    neuron.set_param(neun_py.HRDoubleParameter.S, 4)\n    neuron.set_param(neun_py.HRDoubleParameter.a, 1)\n    neuron.set_param(neun_py.HRDoubleParameter.b, 3)\n    neuron.set_param(neun_py.HRDoubleParameter.c, 1)\n    neuron.set_param(neun_py.HRDoubleParameter.d, 5)\n    neuron.set_param(neun_py.HRDoubleParameter.xr, -1.6)\n    neuron.set_param(neun_py.HRDoubleParameter.vh, 1)\n\n    # HR model uses default parameters, but we can modify them if needed\n    # Set initial conditions\n    neuron.set(neun_py.HRDoubleVariable.x, -0.712841)\n    neuron.set(neun_py.HRDoubleVariable.y, -1.93688) \n    neuron.set(neun_py.HRDoubleVariable.z, 3.16568) \n    \n    # Simulate with different input currents\n    V = []\n    for t in time:\n        neuron.add_synaptic_input(params['I'])\n        neuron.step(dt)\n        V.append(neuron.get(neun_py.HRDoubleVariable.x))  # x is membrane potential\n    \n    # Plot\n    axes[idx].plot(time, V, 'b-', linewidth=1)\n    axes[idx].set_title(f\"{params['name']} (I = {params['I']})\")\n    axes[idx].set_ylabel('Membrane Potential')\n    axes[idx].grid(True, alpha=0.3)\n    if idx == len(parameter_sets) - 1:\n        axes[idx].set_xlabel('Time (ms)')\n\nplt.suptitle('Hindmarsh-Rose Neuron: Different Firing Patterns', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nTipHands-on\n\n\n\n\nTry changing the external current I‚Ä¶ üíª\nCan you observe transitions between regular spiking and chaotic bursting?\n\n\nSolution\n\n# Change I to observe different behaviors\n# I = 2.0  # Regular spiking\nI = 3.2  # Chaotic bursting\n\n\n\n\n\nThe Izhikevich model can reproduce many different firing patterns with just 4 parameters:\n\\[\n\\begin{aligned}\n\\frac{dV}{dt} &= 0.04V^2 + 5V + 140 - u + I \\\\\n\\frac{du}{dt} &= a(bV - u)\n\\end{aligned}\n\\]\nWhen \\(V \\geq 30\\) mV: \\(V \\to c\\), \\(u \\to u + d\\)\n\n\n\nIzhikevich neural activity types. From https://www.izhikevich.org/publications/spikes.htm\n\n\n\n\nsrc/izhikevich.py\n\nimport matplotlib.pyplot as plt\nimport neun_py\n\n# Dictionary of Izhikevich parameters for different cell types\nneuron_types = {\n    'Regular Spiking (RS)': {\n        'a': 0.02, 'b': 0.2, 'c': -65, 'd': 8,\n        'I_amp': 10, 'color': 'blue'\n    },\n    # You could add more neuron types here\n}\n\n# Simulate and plot\ndt = 0.1\nT = 1000\nn_steps = int(T / dt)\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor idx, (name, params) in enumerate(neuron_types.items()):\n    # Create Izhikevich neuron\n    args = neun_py.IzDoubleConstructorArgs()\n    neuron = neun_py.IzDoubleRK4(args)\n\n    # Set parameters\n    neuron.set_param(neun_py.IzDoubleParameter.a, params['a'])\n    neuron.set_param(neun_py.IzDoubleParameter.b, params['b'])\n    neuron.set_param(neun_py.IzDoubleParameter.c, params['c'])\n    neuron.set_param(neun_py.IzDoubleParameter.d, params['d'])\n\n    # Set initial conditions (you can change them if you want)\n    neuron.set(neun_py.IzDoubleVariable.v, -65.0)\n    neuron.set(neun_py.IzDoubleVariable.u, params['b'] * -65.0)\n    \n    V_trace = []\n    t_trace = []\n    \n    for step in range(n_steps):\n        t = step * dt\n        neuron.add_synaptic_input(params['I_amp'])\n        neuron.step(dt)\n        V_trace.append(neuron.get(neun_py.IzDoubleVariable.v))\n        t_trace.append(t)\n    \n    axes[idx].plot(t_trace, V_trace, color=params['color'], linewidth=1.5)\n    axes[idx].set_title(name, fontsize=11, fontweight='bold')\n    axes[idx].set_ylabel('V (mV)')\n    axes[idx].grid(True, alpha=0.3)\n    axes[idx].set_ylim([-80, 40])\n\naxes[-2].set_xlabel('Time (ms)')\naxes[-1].set_xlabel('Time (ms)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nTipHands-on\n\n\n\nNow it‚Äôs your turn! üíª\nTry adding a new set of parameters a, b, c, and d to neuron_types dictionary, then see how they affect the firing patterns. Can you replicate the neuron types shown in the image above?\n\n\nSolution\n\n# Here you have the complete set of parameters for different neuron types\nneuron_types = {\n    'Regular Spiking (RS)': {\n        'a': 0.02, 'b': 0.2, 'c': -65, 'd': 8,\n        'I_amp': 10, 'color': 'blue'\n    },\n    'Intrinsically Bursting (IB)': {\n        'a': 0.02, 'b': 0.2, 'c': -55, 'd': 4,\n        'I_amp': 10, 'color': 'green'\n    },\n    'Chattering (CH)': {\n        'a': 0.02, 'b': 0.2, 'c': -50, 'd': 2,\n        'I_amp': 10, 'color': 'red'\n    },\n    'Fast Spiking (FS)': {\n        'a': 0.1, 'b': 0.2, 'c': -65, 'd': 2,\n        'I_amp': 10, 'color': 'purple'\n    },\n    'Low-Threshold Spiking (LTS)': {\n        'a': 0.02, 'b': 0.25, 'c': -65, 'd': 2,\n        'I_amp': 10, 'color': 'orange'\n    },\n    'Resonator (RZ)': {\n        'a': 0.1, 'b': 0.26, 'c': -65, 'd': 2,\n        'I_amp': 3.5, 'color': 'brown'\n    }\n}\n\n\n\n\n\n\n\n\n\nImportantChoosing Parameters\n\n\n\nIzhikevich parameters:\n\na: Recovery time scale (0.01-0.1)\nb: Sensitivity of u to V (0.2-0.25)\nc: Reset voltage (-65 to -50 mV)\nd: Reset shift of u (2-8)\n\nDifferent combinations produce different firing patterns!\n\n\n\n\n\n\n\n\n\n\n\nImportantChoosing a Neuron Model\n\n\n\n\nHH (Hodgkin-Huxley): Biophysically detailed, captures ion channel dynamics\nHR (Hindmarsh-Rose): Bursting and chaotic behavior, moderate complexity\nIzhikevich: Various firing patterns with simple equations\n\nIntegrator Selection:\n\nRK4: Best balance of accuracy and speed (recommended)\nRK6: Higher accuracy for sensitive systems\nEuler: Fast but less accurate\n\nPrecision: Use Double for most cases, Float for large-scale simulations",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Neurons"
    ]
  },
  {
    "objectID": "02-02-single-neurons.html#visualizing-parameters-and-variables-in-the-model",
    "href": "02-02-single-neurons.html#visualizing-parameters-and-variables-in-the-model",
    "title": "2.2: Modeling Neurons",
    "section": "Visualizing parameters and variables in the model",
    "text": "Visualizing parameters and variables in the model\n\n\nObtaining parameter and variable values\n\nSo far we have plot the voltage at each instant, but in the model we have access to all the parameters that conform the model. Try out this example of Hodgkin-Huxley model showing several variables:\n\n\nsrc/hh-parameters.py\n\nimport neun_py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create neuron\nargs = neun_py.HHDoubleConstructorArgs()\nneuron = neun_py.HHDoubleRK4(args)\n\n# Set parameters\nneuron.set_param(neun_py.HHDoubleParameter.cm, 1.0 * 7.854e-3)\nneuron.set_param(neun_py.HHDoubleParameter.vna, 50.0)\nneuron.set_param(neun_py.HHDoubleParameter.vk, -77.0)\nneuron.set_param(neun_py.HHDoubleParameter.vl, -54.387)\nneuron.set_param(neun_py.HHDoubleParameter.gna, 120 * 7.854e-3)\nneuron.set_param(neun_py.HHDoubleParameter.gk, 36 * 7.854e-3)\nneuron.set_param(neun_py.HHDoubleParameter.gl, 0.3 * 7.854e-3)\n\n# Set initial conditions\nneuron.set(neun_py.HHDoubleVariable.v, -80.0)\nneuron.set(neun_py.HHDoubleVariable.m, 0.1)\nneuron.set(neun_py.HHDoubleVariable.n, 0.7)\nneuron.set(neun_py.HHDoubleVariable.h, 0.01)\n\n# Simulate and record multiple variables\ndt = 0.001\nT = 100\ntime = np.arange(0, T, dt)\n\n# Storage arrays\nV = []      # Membrane potential\nm_vals = [] # Na activation\nh_vals = [] # Na inactivation  \nn_vals = [] # K activation\n\nfor t in time:\n    neuron.add_synaptic_input(0.1)\n    neuron.step(dt)\n    \n    # Record all variables of interest\n    V.append(neuron.get(neun_py.HHDoubleVariable.v))\n    m_vals.append(neuron.get(neun_py.HHDoubleVariable.m))\n    h_vals.append(neuron.get(neun_py.HHDoubleVariable.h))\n    n_vals.append(neuron.get(neun_py.HHDoubleVariable.n))\n\n# Plot multiple variables\nfig, axes = plt.subplots(4, 1, figsize=(10, 10), sharex=True)\n\naxes[0].plot(time, V, 'b-', linewidth=1.5)\naxes[0].set_ylabel('V (mV)')\naxes[0].set_title('Membrane Potential')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(time, m_vals, 'r-', linewidth=1.5)\naxes[1].set_ylabel('m')\naxes[1].set_title('Na Activation')\naxes[1].grid(True, alpha=0.3)\n\naxes[2].plot(time, h_vals, 'g-', linewidth=1.5)\naxes[2].set_ylabel('h')\naxes[2].set_title('Na Inactivation')\naxes[2].grid(True, alpha=0.3)\n\naxes[3].plot(time, n_vals, 'orange', linewidth=1.5)\naxes[3].set_ylabel('n')\naxes[3].set_title('K Activation')\naxes[3].set_xlabel('Time (ms)')\naxes[3].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nParameter exploration\n\nA key use of simulations is exploring parameter space which can be easily done in a model but can be quite of a challenge experimentally.\n\n\n\n\n\n\nTipParameter Exploration Tips\n\n\n\n\nStart with wide ranges to understand overall behavior\nRefine around interesting regions\nVary one parameter at a time initially\nUse multiple metrics (firing rate, CV of ISI, etc.)\nCompare with experimental data when available\n\n\n\n\n\nsrc/parameter-exploration.py\n\nimport neun_py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Explore effect of different sodium conductances\ngna_values = np.linspace(80, 160, 10) * 7.854e-3\nfiring_frequencies = []\n\ndt = 0.001\nT = 500  # Longer simulation for frequency estimation\ntime = np.arange(0, T, dt)\n\nfor gna in gna_values:\n    # Create neuron\n    args = neun_py.HHDoubleConstructorArgs()\n    neuron = neun_py.HHDoubleRK4(args)\n    \n    # Set parameters\n    neuron.set_param(neun_py.HHDoubleParameter.cm, 1.0 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.vna, 50.0)\n    neuron.set_param(neun_py.HHDoubleParameter.vk, -77.0)\n    neuron.set_param(neun_py.HHDoubleParameter.vl, -54.387)\n    neuron.set_param(neun_py.HHDoubleParameter.gna, gna)  # Vary this\n    neuron.set_param(neun_py.HHDoubleParameter.gk, 36 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.gl, 0.3 * 7.854e-3)\n    \n    # Set initial conditions\n    neuron.set(neun_py.HHDoubleVariable.v, -80.0)\n    neuron.set(neun_py.HHDoubleVariable.m, 0.1)\n    neuron.set(neun_py.HHDoubleVariable.n, 0.7)\n    neuron.set(neun_py.HHDoubleVariable.h, 0.01)\n    \n    # Simulate and count spikes\n    V = []\n    spike_count = 0\n    for t in time:\n        neuron.add_synaptic_input(0.15)\n        neuron.step(dt)\n        V.append(neuron.get(neun_py.HHDoubleVariable.v))\n    \n    # Detect spikes (simple threshold crossing)\n    V = np.array(V)\n    spikes = np.where((V[:-1] &lt; 0) & (V[1:] &gt;= 0))[0]\n    \n    # Calculate frequency\n    if len(spikes) &gt; 1:\n        # Use interval between first and last spike\n        freq = (len(spikes) - 1) / ((spikes[-1] - spikes[0]) * dt / 1000)\n    else:\n        freq = 0\n    \n    firing_frequencies.append(freq)\n\n# Plot parameter sweep\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot(gna_values / 7.854e-3, firing_frequencies, 'o-', linewidth=2, markersize=6)\nax.set_xlabel('Na Conductance (mS/cm¬≤)')\nax.set_ylabel('Firing Frequency (Hz)')\nax.set_title('Effect of Sodium Conductance on Firing Frequency')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nSometimes it is also usefull to run the same model several times, varying some parameters in the simulation, e.g., including a noisy input at different seeds. You can design a loop for that purpose and analyze it directly on Python. You have an examples of batching in src/hh-multiple-trials.py and src/parameter-exploration-batch.py",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Neurons"
    ]
  },
  {
    "objectID": "02-02-single-neurons.html#input-patterns",
    "href": "02-02-single-neurons.html#input-patterns",
    "title": "2.2: Modeling Neurons",
    "section": "Input Patterns",
    "text": "Input Patterns\n\nSo far we have just included fixed real (float) values in the example models, but Neun supports various input patterns through add_synaptic_input(), such as ramp current, sinusoidal currents, pulse trains‚Ä¶",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Neurons"
    ]
  },
  {
    "objectID": "02-02-single-neurons.html#noise-and-variability",
    "href": "02-02-single-neurons.html#noise-and-variability",
    "title": "2.2: Modeling Neurons",
    "section": "Noise and Variability",
    "text": "Noise and Variability\nReal neurons operate in noisy environments. Noise affects spiking in two ways, so let‚Äôs explore them. First, we‚Äôll simulate it by adding noise as a synaptic current input. Then, we‚Äôll use the chaotic regime of Hindmarsh-Rose.\n\n\n\n\n\n\nNoteChaos vs Noise\n\n\n\nThe chaotic HR model demonstrates deterministic chaos: irregular behavior arising from deterministic equations without any random input. This is fundamentally different from noise-driven variability:\n\nChaotic: Sensitive dependence on initial conditions, deterministic\nNoisy: Random fluctuations from external sources, stochastic\n\nBoth produce irregular spike patterns, but chaos is intrinsic to the neuron‚Äôs dynamics!\n\n\n\nNoisy Input CurrentChaotic regime\n\n\n\n\nsrc/noisy-input.py\n\nimport neun_py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# -------------------------------------\n# Simulation parameters\n# -------------------------------------\ndt = 0.01\nT = 5000\ntime = np.arange(0, T, dt)\nn_steps = len(time)\n\n# Regular HR mode base current\nI_base = 2.5\nnoise_std = 0.5\n\n# Generate noisy current\nnp.random.seed(42)\nI_clean = np.ones(n_steps) * I_base\nI_noisy = I_base + np.random.randn(n_steps) * noise_std\n\n# -------------------------------------\n# Helper function to configure HR neuron\n# -------------------------------------\ndef configure_hr(neuron):\n    params = neun_py.HRDoubleParameter\n    neuron.set_param(params.e, 0)\n    neuron.set_param(params.mu, 0.006)\n    neuron.set_param(params.S, 4)\n    neuron.set_param(params.a, 1)\n    neuron.set_param(params.b, 3)\n    neuron.set_param(params.c, 1)\n    neuron.set_param(params.d, 5)\n    neuron.set_param(params.xr, -1.6)\n    neuron.set_param(params.vh, 1)\n\n# -------------------------------------\n# Helper function to set initial conditions\n# -------------------------------------\ndef set_initial_conditions(neuron):\n    neuron.set(neun_py.HRDoubleVariable.x, -0.712841)\n    neuron.set(neun_py.HRDoubleVariable.y, -1.93688)\n    neuron.set(neun_py.HRDoubleVariable.z, 3.16568)\n\n# -------------------------------------\n# Simulate HR neuron with arbitrary input\n# -------------------------------------\ndef simulate_current(I_array):\n    neuron = neun_py.HRDoubleRK4(neun_py.HRDoubleConstructorArgs())\n    configure_hr(neuron)\n    set_initial_conditions(neuron)\n\n    V = []\n\n    for k in range(n_steps):\n        # Apply current (clean or noisy)\n        neuron.add_synaptic_input(I_array[k])\n\n        # Integrate one time step\n        neuron.step(dt)\n\n        # Record membrane potential (x variable)\n        V.append(neuron.get(neun_py.HRDoubleVariable.x))\n\n    return np.array(V)\n\n# -------------------------------------\n# Run both simulations\n# -------------------------------------\nV_clean = simulate_current(I_clean)\nV_noisy = simulate_current(I_noisy)\n\n# -------------------------------------\n# Plotting\n# -------------------------------------\nfig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n\naxes[0].plot(time, V_clean, linewidth=1.2, label='Clean Input')\naxes[0].set_ylabel('Membrane Potential (x)')\naxes[0].set_title('HR Regular ‚Äî Clean Input')\naxes[0].grid(True, alpha=0.3)\naxes[0].legend()\n\naxes[1].plot(time, V_noisy, linewidth=1.2, color='orange', label='Noisy Input')\naxes[1].set_xlabel('Time (ms)')\naxes[1].set_ylabel('Membrane Potential (x)')\naxes[1].set_title('HR Regular ‚Äî Noisy Input')\naxes[1].grid(True, alpha=0.3)\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nsrc/chaotic-regime.py\n\nimport neun_py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# -----------------------------\n# Simulation parameters\n# -----------------------------\ndt = 0.01\nT = 5000\ntime = np.arange(0, T, dt)\nn_steps = len(time)\n\n# Base currents\nI_regular = 2.5      # Regular HR\nI_chaotic = 3.2       # Chaotic HR\n\n# -----------------------------\n# HR neuron helpers\n# -----------------------------\ndef configure_hr(neuron):\n    P = neun_py.HRDoubleParameter\n    neuron.set_param(P.e, 0)\n    neuron.set_param(P.mu, 0.006)\n    neuron.set_param(P.S, 4)\n    neuron.set_param(P.a, 1)\n    neuron.set_param(P.b, 3)\n    neuron.set_param(P.c, 1)\n    neuron.set_param(P.d, 5)\n    neuron.set_param(P.xr, -1.6)\n    neuron.set_param(P.vh, 1)\n\ndef set_initial_conditions(neuron):\n    V = neun_py.HRDoubleVariable\n    neuron.set(V.x, -0.712841)\n    neuron.set(V.y, -1.93688)\n    neuron.set(V.z, 3.16568)\n\ndef simulate_hr(I_array):\n    \"\"\"Simulate HR neuron with a given input array\"\"\"\n    neuron = neun_py.HRDoubleRK4(neun_py.HRDoubleConstructorArgs())\n    configure_hr(neuron)\n    set_initial_conditions(neuron)\n\n    V_trace, y_trace, z_trace = [], [], []\n    for I_t in I_array:\n        neuron.add_synaptic_input(I_t)\n        neuron.step(dt)\n        V_trace.append(neuron.get(neun_py.HRDoubleVariable.x))\n        y_trace.append(neuron.get(neun_py.HRDoubleVariable.y))\n        z_trace.append(neuron.get(neun_py.HRDoubleVariable.z))\n\n    return np.array(V_trace), np.array(y_trace), np.array(z_trace)\n\n# -----------------------------\n# Generate input currents\n# -----------------------------\nnp.random.seed(42)\nI_regular_clean = np.ones(n_steps) * I_regular\nI_chaotic_array = np.ones(n_steps) * I_chaotic\n\n# -----------------------------\n# Run simulations\n# -----------------------------\nV_clean, y_clean, z_clean = simulate_hr(I_regular_clean)\nV_chaotic, y_chaotic, z_chaotic = simulate_hr(I_chaotic_array)\n\n# -----------------------------\n# Plot membrane potentials\n# -----------------------------\nfig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n\naxes[0].plot(time, V_clean, color='steelblue', linewidth=1)\naxes[0].set_title(\"HR Regular ‚Äî Clean Input\")\naxes[0].set_ylabel(\"Membrane Potential (x)\")\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(time, V_chaotic, color='darkred', linewidth=1)\naxes[1].set_title(\"HR Chaotic\")\naxes[1].set_xlabel(\"Time (ms)\")\naxes[1].set_ylabel(\"Membrane Potential (x)\")\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# -----------------------------\n# Phase plane comparisons (x-y)\n# -----------------------------\nfig, axes = plt.subplots(1, 2, figsize=(18, 5))\naxes[0].plot(V_clean, y_clean, color='steelblue', linewidth=0.7)\naxes[0].set_title(\"Regular Clean: Phase Space (x-y)\")\naxes[0].set_xlabel(\"x\"); axes[0].set_ylabel(\"y\"); axes[0].grid(True, alpha=0.3)\n\naxes[1].plot(V_chaotic, y_chaotic, color='darkred', linewidth=0.7)\naxes[1].set_title(\"Chaotic: Phase Space (x-y)\")\naxes[1].set_xlabel(\"x\"); axes[1].set_ylabel(\"y\"); axes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# -----------------------------\n# Phase plane comparisons (x-z)\n# -----------------------------\nfig, axes = plt.subplots(1, 2, figsize=(18, 5))\naxes[0].plot(V_clean, z_clean, color='steelblue', linewidth=0.7)\naxes[0].set_title(\"Regular Clean: Phase Space (x-z)\")\naxes[0].set_xlabel(\"x\"); axes[0].set_ylabel(\"z\"); axes[0].grid(True, alpha=0.3)\n\naxes[1].plot(V_chaotic, z_chaotic, color='darkred', linewidth=0.7)\naxes[1].set_title(\"Chaotic: Phase Space (x-z)\")\naxes[1].set_xlabel(\"x\"); axes[1].set_ylabel(\"z\"); axes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# -----------------------------\n# 3D phase space comparison\n# -----------------------------\ntry:\n    from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n    fig = plt.figure(figsize=(18, 5))\n\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax1.plot(V_clean, y_clean, z_clean, color='steelblue', linewidth=0.5, alpha=0.8)\n    ax1.set_title(\"Regular Clean 3D Phase Space\")\n    ax1.set_xlabel(\"x\"); ax1.set_ylabel(\"y\"); ax1.set_zlabel(\"z\")\n\n    ax2 = fig.add_subplot(122, projection='3d')\n    ax2.plot(V_chaotic, y_chaotic, z_chaotic, color='darkred', linewidth=0.5, alpha=0.8)\n    ax2.set_title(\"Chaotic 3D Phase Space\")\n    ax2.set_xlabel(\"x\"); ax2.set_ylabel(\"y\"); ax2.set_zlabel(\"z\")\n\n    plt.tight_layout()\n    plt.show()\nexcept ImportError:\n    print(\"3D plotting not available; skipping 3D phase space.\")\n\n\n\n\n\nAnalyzing the change: Coefficient of Variation (CV) of ISI\nThe CV quantifies spike time variability:\n\n\nsrc/cv-isis.py\n\nimport neun_py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ---------------------------------------------------------\n# Simulation parameters\n# ---------------------------------------------------------\ndt = 0.01\nT = 5000\ntime = np.arange(0, T, dt)\nn_steps = len(time)\n\n# ---------------------------------------------------------\n# HR Parameter Setup\n# ---------------------------------------------------------\ndef configure_hr(neuron):\n    P = neun_py.HRDoubleParameter\n    neuron.set_param(P.e, 0)\n    neuron.set_param(P.mu, 0.006)\n    neuron.set_param(P.S, 4)\n    neuron.set_param(P.a, 1)\n    neuron.set_param(P.b, 3)\n    neuron.set_param(P.c, 1)\n    neuron.set_param(P.d, 5)\n    neuron.set_param(P.xr, -1.6)\n    neuron.set_param(P.vh, 1)\n\ndef set_initial_conditions(neuron):\n    V = neun_py.HRDoubleVariable\n    neuron.set(V.x, -0.712841)\n    neuron.set(V.y, -1.93688)\n    neuron.set(V.z, 3.16568)\n\n# ---------------------------------------------------------\n# HR Simulation Function\n# ---------------------------------------------------------\ndef simulate_HR(I_array):\n    neuron = neun_py.HRDoubleRK4(neun_py.HRDoubleConstructorArgs())\n    configure_hr(neuron)\n    set_initial_conditions(neuron)\n\n    V_trace = []\n    for k in range(n_steps):\n        neuron.add_synaptic_input(I_array[k])\n        neuron.step(dt)\n        V_trace.append(neuron.get(neun_py.HRDoubleVariable.x))\n\n    return np.array(V_trace)\n\n# ---------------------------------------------------------\n# Peak-based Spike Detection (for HR model)\n# ---------------------------------------------------------\ndef compute_ISI_stats_HR(V, threshold=0.0):\n    \"\"\"Detect spikes as local maxima in the HR x-variable.\"\"\"\n    spike_times = []\n\n    for i in range(1, len(V)-1):\n        if V[i] &gt; threshold and V[i] &gt; V[i-1] and V[i] &gt; V[i+1]:\n            # Avoid double-counting\n            if len(spike_times) == 0 or (i*dt - spike_times[-1]) &gt; 5:\n                spike_times.append(i * dt)\n\n    if len(spike_times) &lt; 2:\n        return None, None\n\n    ISIs = np.diff(spike_times)\n    CV = np.std(ISIs) / np.mean(ISIs)\n\n    return ISIs, CV\n\n# ---------------------------------------------------------\n# Generate currents\n# ---------------------------------------------------------\nI_base_regular = 2.5\nnoise_std = 0.5\n\nnp.random.seed(42)\nI_clean = np.ones(n_steps) * I_base_regular\nI_noisy = I_base_regular + np.random.randn(n_steps) * noise_std\n\n# Chaotic HR input\nI_chaotic = 3.2\nI_chaotic_array = np.ones(n_steps) * I_chaotic\n\n# ---------------------------------------------------------\n# Simulations\n# ---------------------------------------------------------\nV_clean = simulate_HR(I_clean)\nV_noisy = simulate_HR(I_noisy)\nV_chaotic = simulate_HR(I_chaotic_array)\n\n# ---------------------------------------------------------\n# Compute ISI statistics\n# ---------------------------------------------------------\nISIs_clean, CV_clean = compute_ISI_stats_HR(V_clean)\nISIs_noisy, CV_noisy = compute_ISI_stats_HR(V_noisy)\nISIs_ch, CV_ch = compute_ISI_stats_HR(V_chaotic)\n\nprint(\"\\n--- CV VALUES ---\")\nprint(f\"HR Regular (clean):   CV = {CV_clean:.3f}\")\nprint(f\"HR Regular (noisy):   CV = {CV_noisy:.3f}\")\nprint(f\"HR Chaotic:           CV = {CV_ch:.3f}\")\n\n# ---------------------------------------------------------\n# Plot Voltage traces\n# ---------------------------------------------------------\nfig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n\naxes[0].plot(time, V_clean, linewidth=1.1)\naxes[0].set_title(\"HR Regular ‚Äî Clean Input\")\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(time, V_noisy, color=\"orange\", linewidth=1.1)\naxes[1].set_title(\"HR Regular ‚Äî Noisy Input\")\naxes[1].grid(True, alpha=0.3)\n\naxes[2].plot(time, V_chaotic, color=\"red\", linewidth=1.1)\naxes[2].set_title(\"HR Chaotic Mode\")\naxes[2].set_xlabel(\"Time (ms)\")\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ---------------------------------------------------------\n# Plot ISI Distributions\n# ---------------------------------------------------------\nplt.figure(figsize=(12, 5))\nplt.hist(ISIs_clean, bins=40, alpha=0.5, label=\"HR Clean\", density=True)\nplt.hist(ISIs_noisy, bins=40, alpha=0.5, label=\"HR Noisy\", density=True)\nplt.hist(ISIs_ch, bins=40, alpha=0.5, label=\"HR Chaotic\", density=True)\nplt.xlabel(\"Inter-Spike Interval (ms)\")\nplt.ylabel(\"Probability Density\")\nplt.title(\"ISI Distributions: Clean vs Noisy vs Chaotic HR\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\nExercises\nTry these exercises to solidify your understanding:\n\nParameter sweep: Create a 2D parameter sweep varying both gna and gk in the HH model\nPhase plane: Plot the (V, n) phase plane for the HH model\nNoise effects: Compare firing patterns with different noise levels in the input current\n\nContinue to 2.3: Modeling Neural Networks",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Neurons"
    ]
  },
  {
    "objectID": "advanced-topics.html",
    "href": "advanced-topics.html",
    "title": "Part 5: Advanced Topics",
    "section": "",
    "text": "In this final section, we‚Äôll explore advanced topics in computational neuroscience including synaptic plasticity, complex network dynamics, and applications to real neuroscience problems."
  },
  {
    "objectID": "advanced-topics.html#introduction",
    "href": "advanced-topics.html#introduction",
    "title": "Part 5: Advanced Topics",
    "section": "",
    "text": "In this final section, we‚Äôll explore advanced topics in computational neuroscience including synaptic plasticity, complex network dynamics, and applications to real neuroscience problems."
  },
  {
    "objectID": "advanced-topics.html#synaptic-plasticity",
    "href": "advanced-topics.html#synaptic-plasticity",
    "title": "Part 5: Advanced Topics",
    "section": "Synaptic Plasticity",
    "text": "Synaptic Plasticity\nSynapses aren‚Äôt static‚Äîthey change based on activity. This plasticity underlies learning and memory.\n\nSpike-Timing-Dependent Plasticity (STDP)\nSTDP adjusts synaptic strength based on the relative timing of pre- and post-synaptic spikes:\n\nPre before post (Œît &gt; 0): Potentiation (strengthening)\nPost before pre (Œît &lt; 0): Depression (weakening)\n\nimport neun\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# STDP learning rule parameters\ntau_plus = 20.0   # Time constant for potentiation (ms)\ntau_minus = 20.0  # Time constant for depression (ms)\nA_plus = 0.01     # Maximum potentiation\nA_minus = 0.01    # Maximum depression\n\ndef stdp_window(dt):\n    \"\"\"STDP learning window.\"\"\"\n    if dt &gt; 0:\n        return A_plus * np.exp(-dt / tau_plus)\n    else:\n        return -A_minus * np.exp(dt / tau_minus)\n\n# Plot STDP window\ndt_range = np.linspace(-100, 100, 200)\ndw = [stdp_window(dt) for dt in dt_range]\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(dt_range, dw, 'b-', linewidth=2)\nax.axhline(0, color='gray', linestyle='--', alpha=0.5)\nax.axvline(0, color='gray', linestyle='--', alpha=0.5)\nax.set_xlabel('Œît = t_post - t_pre (ms)', fontsize=12)\nax.set_ylabel('Weight Change (Œîw)', fontsize=12)\nax.set_title('STDP Learning Window', fontsize=14)\nax.grid(True, alpha=0.3)\nax.text(30, A_plus*0.6, 'LTP\\n(Potentiation)', fontsize=11, ha='center')\nax.text(-30, -A_minus*0.6, 'LTD\\n(Depression)', fontsize=11, ha='center')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nNoteSTDP Principles\n\n\n\n\nCausality: Strengthens synapses that contributed to firing\nLocality: Depends only on local spike timing\nAsymmetry: Direction matters (pre‚Üípost vs post‚Üípre)\nTemporal window: Effective within ~50-100 ms\n\n\n\n\n\nImplementing STDP\nclass STDPSynapse:\n    \"\"\"\n    Simple STDP synapse implementation.\n    \n    Implements spike-timing-dependent plasticity (STDP) for learning\n    temporal correlations between pre and post-synaptic spikes.\n    \n    Parameters:\n        n_pre (int): Number of pre-synaptic neurons\n        n_post (int): Number of post-synaptic neurons\n        w_init (float): Initial synaptic weight (default: 0.5)\n        w_max (float): Maximum synaptic weight (default: 1.0)\n    \n    The STDP rule strengthens synapses when pre-synaptic spikes precede\n    post-synaptic spikes (causality), and weakens them otherwise.\n    \"\"\"\n    \n    def __init__(self, n_pre, n_post, w_init=0.5, w_max=1.0):\n        self.n_pre = n_pre\n        self.n_post = n_post\n        self.w = np.ones((n_post, n_pre)) * w_init\n        self.w_max = w_max\n        \n        # STDP parameters\n        self.A_plus = 0.01\n        self.A_minus = 0.01\n        self.tau_plus = 20.0\n        self.tau_minus = 20.0\n        \n        # Spike traces\n        self.trace_pre = np.zeros(n_pre)\n        self.trace_post = np.zeros(n_post)\n        self.tau_trace = 20.0\n    \n    def update_traces(self, dt, spikes_pre, spikes_post):\n        \"\"\"Update spike traces and apply STDP.\"\"\"\n        # Decay traces\n        self.trace_pre *= np.exp(-dt / self.tau_trace)\n        self.trace_post *= np.exp(-dt / self.tau_trace)\n        \n        # Apply STDP when spikes occur\n        if np.any(spikes_post):\n            # LTD: post spike depresses recently active pre synapses\n            for post_idx in np.where(spikes_post)[0]:\n                self.w[post_idx, :] -= self.A_minus * self.trace_pre\n        \n        if np.any(spikes_pre):\n            # LTP: pre spike potentiates recently active post synapses\n            for pre_idx in np.where(spikes_pre)[0]:\n                self.w[:, pre_idx] += self.A_plus * self.trace_post\n        \n        # Add new spikes to traces\n        self.trace_pre[spikes_pre] = 1.0\n        self.trace_post[spikes_post] = 1.0\n        \n        # Bound weights\n        self.w = np.clip(self.w, 0, self.w_max)\n    \n    def get_current(self, spikes_pre, V_post):\n        \"\"\"Compute synaptic current.\"\"\"\n        # Simplified: instantaneous transmission\n        I_syn = np.dot(self.w, spikes_pre.astype(float))\n        return I_syn\n\n# Example: STDP learning in a simple network\nn_pre = 10\nn_post = 5\n\nstdp_syn = STDPSynapse(n_pre, n_post, w_init=0.3)\n\n# Create pre and post populations\npre_neurons = neun.Population(\n    n_pre,\n    neuron_type=neun.LIFNeuron,\n    C_m=1.0, g_L=0.1, E_L=-70.0,\n    V_th=-50.0, V_reset=-70.0\n)\n\npost_neurons = neun.Population(\n    n_post,\n    neuron_type=neun.LIFNeuron,\n    C_m=1.0, g_L=0.1, E_L=-70.0,\n    V_th=-50.0, V_reset=-70.0\n)\n\n# Training protocol: present patterns\ndt = 0.1\nT = 5000\nn_steps = int(T / dt)\n\n# Pattern: specific pre neurons fire, followed by post\npattern_times = np.arange(100, T, 200)\npattern_pre = [0, 2, 5]  # These pre neurons\npattern_post = [1, 3]     # Should drive these post neurons\n\n# Record initial and final weights\nw_initial = stdp_syn.w.copy()\n\n# Simulate\npre_neurons.reset()\npost_neurons.reset()\n\nfor step in range(n_steps):\n    t = step * dt\n    \n    # External input for pattern presentation\n    I_pre = np.zeros(n_pre)\n    I_post = np.zeros(n_post)\n    \n    # Present pattern\n    for t_pattern in pattern_times:\n        if abs(t - t_pattern) &lt; 5:  # Pre neurons fire\n            I_pre[pattern_pre] = 5.0\n        if abs(t - (t_pattern + 10)) &lt; 5:  # Post neurons fire 10ms later\n            I_post[pattern_post] = 3.0\n    \n    # Step neurons\n    spikes_pre = pre_neurons.step(dt, I_pre)\n    \n    # Synaptic input to post\n    I_syn = stdp_syn.get_current(spikes_pre, post_neurons.V)\n    \n    spikes_post = post_neurons.step(dt, I_post + I_syn)\n    \n    # Update STDP\n    stdp_syn.update_traces(dt, spikes_pre, spikes_post)\n\nw_final = stdp_syn.w.copy()\n\n# Visualize weight changes\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n\nim1 = ax1.imshow(w_initial, aspect='auto', cmap='viridis', vmin=0, vmax=1)\nax1.set_title('Initial Weights')\nax1.set_xlabel('Pre Neuron')\nax1.set_ylabel('Post Neuron')\nplt.colorbar(im1, ax=ax1)\n\nim2 = ax2.imshow(w_final, aspect='auto', cmap='viridis', vmin=0, vmax=1)\nax2.set_title('Final Weights (After STDP)')\nax2.set_xlabel('Pre Neuron')\nax2.set_ylabel('Post Neuron')\nplt.colorbar(im2, ax=ax2)\n\nim3 = ax3.imshow(w_final - w_initial, aspect='auto', cmap='RdBu', vmin=-0.3, vmax=0.3)\nax3.set_title('Weight Change')\nax3.set_xlabel('Pre Neuron')\nax3.set_ylabel('Post Neuron')\nplt.colorbar(im3, ax=ax3, label='Œîw')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Weight changes for trained connections:\")\nfor post_idx in pattern_post:\n    for pre_idx in pattern_pre:\n        dw = w_final[post_idx, pre_idx] - w_initial[post_idx, pre_idx]\n        print(f\"  Pre {pre_idx} ‚Üí Post {post_idx}: {dw:+.3f}\")\n\n\n\n\n\n\nTipSTDP Applications\n\n\n\n\nSequence learning: Learn temporal sequences\nFeature selectivity: Develop receptive fields\nSynchrony: Promote synchronous assemblies\nPruning: Remove unused connections"
  },
  {
    "objectID": "advanced-topics.html#heterogeneous-networks",
    "href": "advanced-topics.html#heterogeneous-networks",
    "title": "Part 5: Advanced Topics",
    "section": "Heterogeneous Networks",
    "text": "Heterogeneous Networks\nReal neural populations are diverse. Let‚Äôs explore heterogeneity:\n# Create heterogeneous network\nN = 100\n\n# Draw parameters from distributions\nC_m_values = np.random.gamma(shape=4, scale=0.25, size=N)\ng_L_values = np.random.gamma(shape=4, scale=0.025, size=N)\nV_th_values = np.random.normal(loc=-50, scale=2, size=N)\n\n# Create neurons with heterogeneous parameters\nneurons = []\nfor i in range(N):\n    neuron = neun.LIFNeuron(\n        C_m=C_m_values[i],\n        g_L=g_L_values[i],\n        E_L=-70.0,\n        V_th=V_th_values[i],\n        V_reset=-70.0\n    )\n    neurons.append(neuron)\n\n# Simulate with same input\ndt = 0.1\nT = 500\ntime = np.arange(0, T, dt)\nI_ext = 2.0  # Constant input\n\nspike_times_het = [[] for _ in range(N)]\n\nfor neuron_idx, neuron in enumerate(neurons):\n    neuron.reset()\n    for i, t in enumerate(time):\n        if neuron.step(dt, I_ext):\n            spike_times_het[neuron_idx].append(t)\n\n# Compute firing rates\nfiring_rates_het = [len(spikes) / (T / 1000) for spikes in spike_times_het]\n\n# Plot heterogeneity effects\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Raster plot\nfor idx in range(N):\n    if spike_times_het[idx]:\n        axes[0, 0].scatter(spike_times_het[idx], [idx] * len(spike_times_het[idx]),\n                          s=1, c='blue', marker='|', alpha=0.5)\naxes[0, 0].set_ylabel('Neuron Index')\naxes[0, 0].set_xlabel('Time (ms)')\naxes[0, 0].set_title('Heterogeneous Population Activity')\n\n# Firing rate distribution\naxes[0, 1].hist(firing_rates_het, bins=20, edgecolor='black', alpha=0.7)\naxes[0, 1].set_xlabel('Firing Rate (Hz)')\naxes[0, 1].set_ylabel('Count')\naxes[0, 1].set_title('Firing Rate Distribution')\naxes[0, 1].axvline(np.mean(firing_rates_het), color='r', \n                   linestyle='--', label=f'Mean = {np.mean(firing_rates_het):.1f} Hz')\naxes[0, 1].legend()\n\n# Parameter distributions\naxes[1, 0].hist(C_m_values, bins=20, alpha=0.7, edgecolor='black')\naxes[1, 0].set_xlabel('Membrane Capacitance (nF)')\naxes[1, 0].set_ylabel('Count')\naxes[1, 0].set_title('Parameter Heterogeneity: C_m')\n\naxes[1, 1].hist(V_th_values, bins=20, alpha=0.7, edgecolor='black')\naxes[1, 1].set_xlabel('Threshold (mV)')\naxes[1, 1].set_ylabel('Count')\naxes[1, 1].set_title('Parameter Heterogeneity: V_th')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nImportantWhy Heterogeneity Matters\n\n\n\n\nBiological realism: Real neurons vary\nRobustness: Diversity can improve stability\nDynamic range: Broader input encoding\nIrregular firing: More realistic spike patterns"
  },
  {
    "objectID": "advanced-topics.html#modeling-specific-brain-regions",
    "href": "advanced-topics.html#modeling-specific-brain-regions",
    "title": "Part 5: Advanced Topics",
    "section": "Modeling Specific Brain Regions",
    "text": "Modeling Specific Brain Regions\n\nExample: Simplified Cortical Microcircuit\nBased on cortical column architecture:\n# Cortical microcircuit layers\nlayer_sizes = {\n    'L2/3_E': 40,\n    'L2/3_I': 10,\n    'L4_E': 30,\n    'L4_I': 8,\n    'L5_E': 35,\n    'L5_I': 9\n}\n\n# Create populations\npopulations = {}\nfor layer_name, size in layer_sizes.items():\n    is_inhibitory = 'I' in layer_name\n    \n    if is_inhibitory:\n        # Fast-spiking inhibitory\n        pop = neun.Population(\n            size,\n            neuron_type=neun.LIFNeuron,\n            C_m=0.5, g_L=0.1, E_L=-70.0,\n            V_th=-50.0, V_reset=-65.0, tau_ref=1.0\n        )\n    else:\n        # Regular-spiking excitatory\n        pop = neun.Population(\n            size,\n            neuron_type=neun.LIFNeuron,\n            C_m=1.0, g_L=0.1, E_L=-70.0,\n            V_th=-50.0, V_reset=-70.0, tau_ref=2.0\n        )\n    \n    populations[layer_name] = pop\n\nprint(\"Cortical microcircuit created:\")\nfor name, pop in populations.items():\n    print(f\"  {name}: {pop.size} neurons\")\n\n# Connection probabilities (simplified from Potjans & Diesmann, 2014)\nconnections = [\n    ('L4_E', 'L2/3_E', 0.15),\n    ('L4_E', 'L2/3_I', 0.10),\n    ('L2/3_E', 'L5_E', 0.05),\n    ('L2/3_I', 'L2/3_E', 0.20),\n    ('L4_I', 'L4_E', 0.25),\n    ('L5_I', 'L5_E', 0.20),\n]\n\n# Create synapses (simplified)\nsynapses_cortex = []\nfor pre_name, post_name, p_conn in connections:\n    is_inhibitory = 'I' in pre_name\n    \n    if is_inhibitory:\n        g_syn = 0.05\n        E_syn = -80.0\n        tau_syn = 8.0\n    else:\n        g_syn = 0.02\n        E_syn = 0.0\n        tau_syn = 5.0\n    \n    syn = neun.ExponentialSynapse(\n        pre_pop=populations[pre_name],\n        post_pop=populations[post_name],\n        g_syn=g_syn,\n        E_syn=E_syn,\n        tau_syn=tau_syn,\n        p=p_conn\n    )\n    synapses_cortex.append(syn)\n\nprint(f\"\\nCreated {len(synapses_cortex)} connection types\")"
  },
  {
    "objectID": "advanced-topics.html#frequency-dependent-processing",
    "href": "advanced-topics.html#frequency-dependent-processing",
    "title": "Part 5: Advanced Topics",
    "section": "Frequency-Dependent Processing",
    "text": "Frequency-Dependent Processing\nRelating to the research of Garrido-Pe√±a et al.¬†(2014), let‚Äôs explore frequency filtering:\n# Test network response to inputs at different frequencies\ndef test_frequency_response(network, frequencies, amplitude=1.0):\n    \"\"\"Test network response to sinusoidal inputs.\"\"\"\n    dt = 0.1\n    T = 2000\n    time = np.arange(0, T, dt)\n    \n    responses = []\n    \n    for freq in frequencies:\n        # Sinusoidal input\n        I_ext = amplitude * (1 + np.sin(2 * np.pi * freq * time / 1000))\n        \n        # Apply to subset of neurons\n        I_ext_full = np.zeros((network.n_neurons, len(time)))\n        I_ext_full[:20, :] = I_ext\n        \n        # Simulate\n        spike_times = [[] for _ in range(network.n_neurons)]\n        network.reset()\n        \n        for step in range(len(time)):\n            fired = network.step(dt, I_ext_full[:, step])\n            for neuron_idx in np.where(fired)[0]:\n                spike_times[neuron_idx].append(time[step])\n        \n        # Compute population response\n        _, rate = compute_population_rate(spike_times, T, dt, window=50)\n        \n        # Measure response amplitude at input frequency\n        from scipy.fft import rfft, rfftfreq\n        \n        fft_rate = np.abs(rfft(rate - np.mean(rate)))\n        fft_freqs = rfftfreq(len(rate), dt / 1000)\n        \n        # Find response at input frequency\n        freq_idx = np.argmin(np.abs(fft_freqs - freq))\n        response_amp = fft_rate[freq_idx]\n        \n        responses.append(response_amp)\n    \n    return responses\n\n# Test with E-I network\nfrequencies_test = [2, 5, 10, 20, 40, 60, 80, 100]\n\n# (Assuming we have a network from previous sections)\n# responses = test_frequency_response(network, frequencies_test)\n\n# Plot frequency response curve\n# fig, ax = plt.subplots(figsize=(10, 6))\n# ax.plot(frequencies_test, responses, 'o-', linewidth=2, markersize=8)\n# ax.set_xlabel('Input Frequency (Hz)', fontsize=12)\n# ax.set_ylabel('Response Amplitude', fontsize=12)\n# ax.set_title('Network Frequency Response', fontsize=14)\n# ax.grid(True, alpha=0.3)\n# ax.set_xscale('log')\n# plt.tight_layout()\n# plt.show()\n\n\n\n\n\n\nNoteFrequency Filtering in Neural Circuits\n\n\n\nAs demonstrated by Garrido-Pe√±a et al.¬†(2014):\n\nDifferent brain regions have different frequency preferences\nCircuit architecture determines filtering properties\nSynaptic time constants shape frequency response\nComputational models help interpret experimental measurements"
  },
  {
    "objectID": "advanced-topics.html#connecting-to-experimental-data",
    "href": "advanced-topics.html#connecting-to-experimental-data",
    "title": "Part 5: Advanced Topics",
    "section": "Connecting to Experimental Data",
    "text": "Connecting to Experimental Data\n\nFitting Models to Data\ndef fit_neuron_model(spike_times_data, I_applied, dt=0.1):\n    \"\"\"\n    Simple parameter fitting example.\n    In practice, use sophisticated optimization methods.\n    \"\"\"\n    from scipy.optimize import differential_evolution\n    \n    def objective(params):\n        \"\"\"Objective function: match spike count and pattern.\"\"\"\n        C_m, g_L, V_th = params\n        \n        # Create neuron with these parameters\n        neuron = neun.LIFNeuron(\n            C_m=C_m, g_L=g_L, E_L=-70.0,\n            V_th=V_th, V_reset=-70.0, tau_ref=2.0\n        )\n        \n        # Simulate\n        spike_times_model = []\n        neuron.reset()\n        \n        for i, I in enumerate(I_applied):\n            if neuron.step(dt, I):\n                spike_times_model.append(i * dt)\n        \n        # Compare with data\n        # Simple metric: difference in spike count\n        error = abs(len(spike_times_data) - len(spike_times_model))\n        \n        # Add ISI distribution error if enough spikes\n        if len(spike_times_data) &gt; 2 and len(spike_times_model) &gt; 2:\n            ISI_data = np.diff(spike_times_data)\n            ISI_model = np.diff(spike_times_model)\n            error += abs(np.mean(ISI_data) - np.mean(ISI_model))\n        \n        return error\n    \n    # Parameter bounds\n    bounds = [\n        (0.5, 2.0),    # C_m\n        (0.05, 0.2),   # g_L\n        (-55.0, -45.0) # V_th\n    ]\n    \n    # Optimize\n    result = differential_evolution(objective, bounds, maxiter=50, \n                                   popsize=10, seed=42)\n    \n    return result.x\n\n# Example usage (with synthetic data)\ndt = 0.1\nT = 1000\ntime = np.arange(0, T, dt)\nI_applied = np.ones(len(time)) * 2.5\n\n# \"Experimental\" data (from known params)\ntrue_neuron = neun.LIFNeuron(\n    C_m=1.2, g_L=0.12, E_L=-70.0,\n    V_th=-52.0, V_reset=-70.0, tau_ref=2.0\n)\n\nspike_times_data = []\ntrue_neuron.reset()\nfor i, I in enumerate(I_applied):\n    if true_neuron.step(dt, I):\n        spike_times_data.append(i * dt)\n\nprint(f\"True parameters: C_m=1.2, g_L=0.12, V_th=-52.0\")\nprint(f\"Experimental spikes: {len(spike_times_data)}\")\n\n# Fit model\n# fitted_params = fit_neuron_model(spike_times_data, I_applied)\n# print(f\"Fitted parameters: C_m={fitted_params[0]:.2f}, \"\n#       f\"g_L={fitted_params[1]:.3f}, V_th={fitted_params[2]:.1f}\")"
  },
  {
    "objectID": "advanced-topics.html#best-practices-for-research",
    "href": "advanced-topics.html#best-practices-for-research",
    "title": "Part 5: Advanced Topics",
    "section": "Best Practices for Research",
    "text": "Best Practices for Research\n\n1. Model Validation\ndef validate_model(model, experimental_metrics):\n    \"\"\"Check if model matches experimental observations.\"\"\"\n    \n    validations = {}\n    \n    # Check firing rate\n    if 'firing_rate' in experimental_metrics:\n        model_rate = compute_model_firing_rate(model)\n        exp_rate = experimental_metrics['firing_rate']\n        validations['firing_rate'] = abs(model_rate - exp_rate) / exp_rate &lt; 0.2\n    \n    # Check CV of ISI\n    if 'CV_ISI' in experimental_metrics:\n        model_CV = compute_model_CV(model)\n        exp_CV = experimental_metrics['CV_ISI']\n        validations['CV_ISI'] = abs(model_CV - exp_CV) &lt; 0.3\n    \n    return validations\n\n\n2. Parameter Sensitivity Analysis\ndef sensitivity_analysis(base_params, param_name, param_range):\n    \"\"\"Test how output varies with parameter changes.\"\"\"\n    \n    results = []\n    \n    for param_value in param_range:\n        params = base_params.copy()\n        params[param_name] = param_value\n        \n        # Run simulation\n        output = run_simulation(params)\n        results.append(output)\n    \n    return results\n\n# Example\n# base = {'C_m': 1.0, 'g_L': 0.1, 'E_L': -70.0, 'V_th': -50.0, 'V_reset': -70.0}\n# g_L_range = np.linspace(0.05, 0.2, 10)\n# results = sensitivity_analysis(base, 'g_L', g_L_range)\n\n\n3. Reproducibility\n# Always set random seeds\nnp.random.seed(42)\n\n# Document parameters clearly\nsimulation_params = {\n    'dt': 0.1,\n    'T': 1000,\n    'network_size': 100,\n    'connection_probability': 0.2,\n    'version': '1.0',\n    'date': '2025-01-15'\n}\n\n# Save results\nimport json\n\nresults = {\n    'parameters': simulation_params,\n    'outputs': {\n        'firing_rates': firing_rates_het,\n        'synchrony': sync_E\n    }\n}\n\nwith open('simulation_results.json', 'w') as f:\n    json.dump(results, f, indent=2)"
  },
  {
    "objectID": "advanced-topics.html#real-world-applications",
    "href": "advanced-topics.html#real-world-applications",
    "title": "Part 5: Advanced Topics",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\n1. Understanding Diseases\nNeural models help understand pathological conditions:\n\nEpilepsy: Excessive synchronization\nParkinson‚Äôs: Abnormal oscillations\nSchizophrenia: E-I imbalance\n\n\n\n2. Brain-Computer Interfaces\nModels inform BCI design:\n\nDecoding neural activity\nPredicting responses to stimulation\nOptimizing electrode placement\n\n\n\n3. Drug Development\nComputational screening:\n\nPredict effects of drugs on neural dynamics\nOptimize dosing regimens\nReduce animal testing"
  },
  {
    "objectID": "advanced-topics.html#summary",
    "href": "advanced-topics.html#summary",
    "title": "Part 5: Advanced Topics",
    "section": "Summary",
    "text": "Summary\nIn this advanced section, we explored:\n\n‚úÖ Synaptic plasticity and STDP\n‚úÖ Heterogeneous networks\n‚úÖ Modeling specific brain circuits\n‚úÖ Frequency-dependent processing\n‚úÖ Fitting models to data\n‚úÖ Validation and sensitivity analysis\n‚úÖ Real-world applications\n\n\n\n\n\n\n\nNoteFinal Thoughts\n\n\n\nComputational neuroscience is a powerful tool for:\n\nUnderstanding: How neural circuits work\nPredicting: Network behavior under various conditions\nDesigning: Experiments and interventions\nIntegrating: Knowledge across scales\nTranslating: Basic science to clinical applications"
  },
  {
    "objectID": "advanced-topics.html#where-to-go-from-here",
    "href": "advanced-topics.html#where-to-go-from-here",
    "title": "Part 5: Advanced Topics",
    "section": "Where to Go From Here",
    "text": "Where to Go From Here\n\nContinue Learning\n\nAdvanced textbooks: Dayan & Abbott, Gerstner et al.\nOnline courses: Neuromatch Academy, Coursera\nResearch papers: Follow current literature\nCoding practice: Implement papers you read\n\n\n\nJoin the Community\n\nNeun GitHub: Contribute, ask questions\nComputational Neuroscience conferences\nOnline forums and discussion groups\n\n\n\nApply Your Skills\n\nAnalyze experimental data\nDevelop new models\nCollaborate with experimentalists\nPublish your findings"
  },
  {
    "objectID": "advanced-topics.html#final-exercise",
    "href": "advanced-topics.html#final-exercise",
    "title": "Part 5: Advanced Topics",
    "section": "Final Exercise",
    "text": "Final Exercise\nBuild a complete model addressing a neuroscience question:\n\nChoose a phenomenon (e.g., working memory, decision making)\nReview relevant literature\nDesign a minimal model\nImplement and test\nCompare with experimental data\nDraw conclusions"
  },
  {
    "objectID": "advanced-topics.html#conclusion",
    "href": "advanced-topics.html#conclusion",
    "title": "Part 5: Advanced Topics",
    "section": "Conclusion",
    "text": "Conclusion\nYou now have the tools to:\n\n‚úÖ Understand computational neuroscience principles\n‚úÖ Build single neuron and network models with Neun\n‚úÖ Simulate complex neural dynamics\n‚úÖ Analyze and interpret results\n‚úÖ Connect models to experimental work\n‚úÖ Apply these skills to research questions\n\nThank you for participating in this workshop!\nWe hope you found it valuable and wish you success in your computational neuroscience journey."
  },
  {
    "objectID": "advanced-topics.html#references-and-resources",
    "href": "advanced-topics.html#references-and-resources",
    "title": "Part 5: Advanced Topics",
    "section": "References and Resources",
    "text": "References and Resources\n\nBooks\n\nDayan, P., & Abbott, L. F. (2001). Theoretical Neuroscience. MIT Press.\nGerstner, W., et al.¬†(2014). Neuronal Dynamics. Cambridge University Press.\nErmentrout, G. B., & Terman, D. H. (2010). Mathematical Foundations of Neuroscience. Springer.\n\n\n\nOnline Resources\n\nNeun Library\nNeuromatch Academy\nBrian Simulator\nNEST Simulator\n\n\n\nContact\nFor questions about this workshop:\n\nDr.¬†Angel Lareo\nDr.¬†Alicia Garrido-Pe√±a\n\nWorkshop repository: github.com/angellareo/workshop-neun\n\n¬°Gracias! Thank you! üß†‚ú®"
  },
  {
    "objectID": "03-conclusions.html",
    "href": "03-conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Computational neuroscience provides a framework for understanding the brain by combining different approaches: mathematics, computer science, and theoretical analysis allow us to understand the principles that govern the development, structure, physiology and cognitive abilities of the nervous system.\nBy abstracting complex neural systems into models, we can uncover principles that govern brain function, predict behavior, and guide experiments in ways that would be difficult with observation alone.\nKey insights include:\nUltimately, computational models serve as a bridge between intuition and experiment, allowing us to explore the principles of neural computation, predict outcomes, and uncover mechanisms that underlie cognition and behavior. Embracing these approaches equips researchers with a versatile toolkit for both discovery and innovation in neuroscience.",
    "crumbs": [
      "Part 3: Conclusions"
    ]
  },
  {
    "objectID": "03-conclusions.html#advanced-topics",
    "href": "03-conclusions.html#advanced-topics",
    "title": "Conclusions",
    "section": "Advanced topics",
    "text": "Advanced topics\nThere are some advanced topics that we have not covered in this workshop, but that you can explore on your own:\n\nFitting to experimental data: Use optimization techniques to fit model parameters to real neural recordings.\nLarge-scale simulations: Explore how to simulate larger networks efficiently using parallel computing.\nPlasticity and learning rules: Implement synaptic plasticity mechanisms to study how synapses adapt.",
    "crumbs": [
      "Part 3: Conclusions"
    ]
  },
  {
    "objectID": "03-conclusions.html#neun",
    "href": "03-conclusions.html#neun",
    "title": "Conclusions",
    "section": "Neun",
    "text": "Neun\nAs you now know, Neun is an open-source tool for modeling and efficient simulation.\nWe have seen along the workshop how to use Neun to create and simulate neurons and simple neural networks, how to analyze the results, and how to visualize the dynamics.\n\nNow is your turn! ü´µ\nNow you have the power to use Neun, so use it wisely ;)\nAnd remember, you can also contribute to the project via pull request (for example, to include any improvement that you may want to use).\nIf you want to include a new model or use Neun in C++ check out this example repository: github.com/GNB-UAM/Neun-Example\nContact us if you have any questions or if you want to develop something together!\nangel.lareo@uam.es \nalicia.garrido@uam.es",
    "crumbs": [
      "Part 3: Conclusions"
    ]
  },
  {
    "objectID": "step-order.html",
    "href": "step-order.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "step-order.html#causal-update-order",
    "href": "step-order.html#causal-update-order",
    "title": "",
    "section": "1. Causal Update Order",
    "text": "1. Causal Update Order\nWhen you step a synapse, it computes the synaptic current based on the current state of the connected neurons (their voltages). This synaptic current then influences the neurons in the next time step. By stepping synapses first:\n# Time t ‚Üí t+dt\n\n# Step 1: Compute synaptic currents based on neuron states at time t\nsynapse.step(dt)  # Computes I_syn(t) from V1(t) and V2(t)\n\n# Step 2: Neurons integrate, including the synaptic current just computed\nneuron1.step(dt)  # Updates V1(t) ‚Üí V1(t+dt) using I_syn(t)\nneuron2.step(dt)  # Updates V2(t) ‚Üí V2(t+dt) using I_syn(t)"
  },
  {
    "objectID": "step-order.html#consistent-coupling",
    "href": "step-order.html#consistent-coupling",
    "title": "",
    "section": "2. Consistent Coupling",
    "text": "2. Consistent Coupling\nFor electrical synapses, the coupling current is: \\[I_{syn}(t) = g_{gap} \\cdot (V_1(t) - V_2(t))\\]\nIf you step neurons first, the synapse would compute its current using the new voltages from time t+dt, but those voltages were computed without considering the synaptic coupling - creating an inconsistency."
  },
  {
    "objectID": "step-order.html#avoiding-feedback-loops",
    "href": "step-order.html#avoiding-feedback-loops",
    "title": "",
    "section": "3. Avoiding Feedback Loops",
    "text": "3. Avoiding Feedback Loops\nWrong order can create artificial feedback:\n# WRONG ORDER - can cause instability\nneuron1.step(dt)  # V1 changes without considering synapse\nneuron2.step(dt)  # V2 changes without considering synapse\nsynapse.step(dt)  # Computes current from already-updated voltages\n                  # This current won't affect neurons until NEXT iteration!\n\n4. Synchronization Between Synapses\nWhen you have multiple synapses, stepping all synapses first ensures they all see the same ‚Äúsnapshot‚Äù of the network state:\n# All synapses evaluate at the same network state (time t)\ns12.step(dt)\ns23.step(dt)\ns31.step(dt)\n\n# Then all neurons update together (t ‚Üí t+dt)\nn1.step(dt)\nn2.step(dt)\nn3.step(dt)\nThis is actually a general principle in parallel simulation - you want to maintain the illusion that everything happens simultaneously within each time step, even though you‚Äôre computing things sequentially."
  },
  {
    "objectID": "input-patterns.html",
    "href": "input-patterns.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "input-patterns.html#input-patterns",
    "href": "input-patterns.html#input-patterns",
    "title": "",
    "section": "Input Patterns",
    "text": "Input Patterns\nNeun supports various input patterns through add_synaptic_input().\n\nConstant Current\n# Applied at each time step\nfor t in time:\n    neuron.add_synaptic_input(2.0)  # Constant 2.0 nA\n    neuron.step(dt)\n\n\nTime-Varying Current\n# Step current\nfor i, t in enumerate(time):\n    if 50 &lt;= t &lt; 150:  # Step from 50 to 150 ms\n        neuron.add_synaptic_input(3.0)\n    else:\n        neuron.add_synaptic_input(0.0)\n    neuron.step(dt)\n\n\nRamp Current\n# Linearly increasing current\nI_max = 5.0\nfor i, t in enumerate(time):\n    I_current = (I_max * t) / time[-1]\n    neuron.add_synaptic_input(I_current)\n    neuron.step(dt)\n\n\nNoisy Current\n# Gaussian white noise\nI_base = 1.0\nsigma = 0.5\nfor t in time:\n    I_noisy = I_base + sigma * np.random.randn()\n    neuron.add_synaptic_input(I_noisy)\n    neuron.step(dt)\n\n\nSinusoidal Current\n# Oscillatory input\nfrequency = 10  # Hz\nI_amplitude = 1.0\nI_offset = 1.5\nfor t in time:\n    I_sin = I_offset + I_amplitude * np.sin(2 * np.pi * frequency * t / 1000)\n    neuron.add_synaptic_input(I_sin)\n    neuron.step(dt)\n\n\nPulse Train\n# Brief current pulses\npulse_times = [50, 100, 150, 200]  # ms\npulse_duration = 5  # ms\npulse_amplitude = 3.0  # nA\n\nfor t in time:\n    I_pulse = 0.0\n    for t_pulse in pulse_times:\n        if t_pulse &lt;= t &lt; t_pulse + pulse_duration:\n            I_pulse = pulse_amplitude\n            break\n    neuron.add_synaptic_input(I_pulse)\n    neuron.step(dt)"
  },
  {
    "objectID": "02-01-neun-basics.html",
    "href": "02-01-neun-basics.html",
    "title": "2.1 Getting Started with Neun",
    "section": "",
    "text": "Neun is a high-performance C++ library for simulating dynamical systems, aimed at modeling neural networks.\nThe neun_py package provides Python bindings to use Neun, giving users access to Neun‚Äôs efficient C++ implementation in Python.\n\n\n\nHigh performance: C++ core with optimized numerical integrators\nType safe: Automatic enum generation for variables and parameters\nFlexible: Multiple neuron models (Hodgkin-Huxley, Hindmarsh-Rose, Izhikevich, etc.)\nMultiple integrators: RungeKutta4, RungeKutta6, Euler, and Stepper\nPrecision options: Both float and double precision support\nHeader-Only C++ library: Easy integration in C++ projects",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Intro to Neun"
    ]
  },
  {
    "objectID": "02-01-neun-basics.html#introduction-to-neun",
    "href": "02-01-neun-basics.html#introduction-to-neun",
    "title": "2.1 Getting Started with Neun",
    "section": "",
    "text": "Neun is a high-performance C++ library for simulating dynamical systems, aimed at modeling neural networks.\nThe neun_py package provides Python bindings to use Neun, giving users access to Neun‚Äôs efficient C++ implementation in Python.\n\n\n\nHigh performance: C++ core with optimized numerical integrators\nType safe: Automatic enum generation for variables and parameters\nFlexible: Multiple neuron models (Hodgkin-Huxley, Hindmarsh-Rose, Izhikevich, etc.)\nMultiple integrators: RungeKutta4, RungeKutta6, Euler, and Stepper\nPrecision options: Both float and double precision support\nHeader-Only C++ library: Easy integration in C++ projects",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Intro to Neun"
    ]
  },
  {
    "objectID": "02-01-neun-basics.html#installation-and-setup",
    "href": "02-01-neun-basics.html#installation-and-setup",
    "title": "2.1 Getting Started with Neun",
    "section": "Installation and Setup",
    "text": "Installation and Setup\n\nInstalling Neun\nFirst, ensure you have Python 3.8+ and a C++ compiler with C++20 support (GCC 10+, Clang 10+). You‚Äôll need to install the Neun C++ library first, then build the Python bindings.\n\n\n\n\n\n\nTipUsing Virtual Environments\n\n\n\nIt‚Äôs recommended to use a virtual environment for the workshop:\npython -m venv neun-workshop\nsource neun-workshop/bin/activate  # On Windows: neun-workshop\\Scripts\\activate\npip install -r requirements.txt\n\n\n\nInstalling the C++ Library\n# Clone and build Neun\ngit clone https://github.com/GNB-UAM/Neun.git\ncd Neun\nmkdir build && cd build\ncmake ..\nmake\nsudo make install\n\n\nInstalling Python Bindings\n# Clone and build neun_py\ngit clone https://github.com/GNB-UAM/neun_py.git\ncd neun_py\nmake  # Generates code and installs in editable mode\n\nLet‚Äôs check that everything (both the C++ library and the Python bindings) is installed correctly.\n\n\n\nVerifying Installation of the Neun C++ library\necho '#include &lt;DifferentialNeuronWrapper.h&gt;\n#include &lt;HodgkinHuxleyModel.h&gt;\n#include &lt;RungeKutta4.h&gt;\n#include &lt;SystemWrapper.h&gt;\n#include &lt;iostream&gt;\ntypedef RungeKutta4 Integrator;\ntypedef DifferentialNeuronWrapper&lt;SystemWrapper&lt;HodgkinHuxleyModel&lt;double&gt;&gt;, Integrator&gt;\n    Neuron;\n\nint main() {\n    // Struct to initialize neuron model parameters\n    Neuron::ConstructorArgs args;\n\n    // Initialize a new neuron model\n    Neuron n(args);\n\n    std::cout &lt;&lt; \"Neun C++ library loaded successfully!\" &lt;&lt; std::endl;\n    return 0;\n}' &gt; test_neun.cpp\ng++ -std=c++20 test_neun.cpp -o test_neun -I/usr/local/Neun/0.4.0/\n# clang++ -std=c++20 test_neun.cpp -o test_neun -I/usr/local/Neun/0.4.0/  # If using Clang\n./test_neun\n\n\n\n\n\n\nTipUse an appropriate C++ compiler\n\n\n\nMake sure to use a C++ compiler that supports C++20 (e.g., GCC 10+, Clang 10+).\nIf you are using Windows, consider installing MinGW or using WSL for a Linux-like environment.\nIf you are using MacOS, you may need to install GCC/Clang via Homebrew: brew install gcc\n\n\n\n\nVerifying Installation of the neun_py Python Bindings\nNow, let‚Äôs check that you can use the python bindings:\npython -c 'import neun_py\nimport numpy as np\n\n# Check available neuron models\nneurons = neun_py.get_available_neurons()\nprint(f\"Available neuron types: {neurons}\")\n\n# Check available synapses\nsynapses = neun_py.get_available_synapses()\nprint(f\"Available synapse types: {synapses}\")\n\nprint(\"\\nNeun Python bindings loaded successfully!\")'\n\n\n\n\n\n\nTipRemember to use the Virtual Environment\n\n\n\nIf you set up a virtual environment, remember to activate it before running your Python scripts after this workshop:\nsource neun-workshop/bin/activate  # On Windows: neun-workshop\\Scripts\\activate",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Intro to Neun"
    ]
  },
  {
    "objectID": "02-01-neun-basics.html#neun-architecture-overview",
    "href": "02-01-neun-basics.html#neun-architecture-overview",
    "title": "2.1 Getting Started with Neun",
    "section": "Neun Architecture Overview",
    "text": "Neun Architecture Overview\nNeun is organized around several key concepts:\n\n1. Neurons\nIndividual computational units that integrate inputs and generate outputs. Neun provides several biophysically-based neuron models.\nAvailable neuron models:\n\n\n\nModel\nShort Name\nDescription\n\n\n\n\nHodgkin-Huxley\nHH\nClassic conductance-based model\n\n\nHindmarsh-Rose\nHR\nSimplified bursting model\n\n\nIzhikevich\nIz\nIzhikevich spiking neuron model (2003)\n\n\nMatsuoka\n\nMatsuoka oscillator (1985)\n\n\nRowatSelverston\n\nRowat and Selverston model (1997)\n\n\nRulkov\n\nRulkov Map model (2002)\n\n\nVavoulis\n\nVavoulis model (2007)\n\n\n\nNaming convention: Models are named as {Model}{Precision}{Integrator} - Example: HHDoubleRK4 = Hodgkin-Huxley, Double precision, RungeKutta4\n\n\n2. Synapses\nConnections between neurons that transmit signals.\nAvailable synapse types:\n\nDiffusion Synapse: Based on Destexhe et al.¬†1994\nElectrical Synapse (ESyn): Gap junction coupling \n\nNaming convention: {SynapseType}{Neuron1}{Neuron2}{Precision}{Integrator} - Example: ESynHHHHDoubleRK4 = Electrical synapse between two HH neurons\n\n\n3. Integrators\nNumerical methods for solving differential equations.\nAvailable integrators:\n\nRK4: 4th-order Runge-Kutta (most common)\nRK6: 6th-order Runge-Kutta (higher accuracy)\nEuler: Simple Euler method\nStepper: Basic stepping\n\n\n\n4. Parameters and Variables\nNeun uses type-safe enums for accessing neuron properties:\n\nParameters: Model constants (e.g., conductances, capacitances)\n\nSet value using neuron.set_param(enum, value)\n\nVariables: Dynamic state (e.g., membrane potential, gating variables)\n\nSet value using neuron.set(enum, value) or neuron.set(enum, value)\nGet using neuron.get(enum)\n\n\nExample enums: HHDoubleParameter.cm, HHDoubleVariable.v",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Intro to Neun"
    ]
  },
  {
    "objectID": "02-01-neun-basics.html#your-first-neun-program",
    "href": "02-01-neun-basics.html#your-first-neun-program",
    "title": "2.1 Getting Started with Neun",
    "section": "Your First Neun Program",
    "text": "Your First Neun Program\nLet‚Äôs create a simple Hodgkin-Huxley neuron simulation to understand the basic workflow:\n\n\nsrc/first-program.py\n\nimport neun_py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Create constructor arguments\nargs = neun_py.HHDoubleConstructorArgs()\n\n# Step 2: Create a Hodgkin-Huxley neuron with Double precision and RK4 integrator\nneuron = neun_py.HHDoubleRK4(args)\n\n# Step 3: Set parameter values\nneuron.set_param(neun_py.HHDoubleParameter.cm, 1.0 * 7.854e-3)    # Capacitance\nneuron.set_param(neun_py.HHDoubleParameter.vna, 50.0)            # Na reversal\nneuron.set_param(neun_py.HHDoubleParameter.vk, -77.0)            # K reversal  \nneuron.set_param(neun_py.HHDoubleParameter.vl, -54.387)          # Leak reversal\nneuron.set_param(neun_py.HHDoubleParameter.gna, 120 * 7.854e-3)  # Na conductance\nneuron.set_param(neun_py.HHDoubleParameter.gk, 36 * 7.854e-3)    # K conductance\nneuron.set_param(neun_py.HHDoubleParameter.gl, 0.3 * 7.854e-3)   # Leak conductance\n\n# Step 4: Set initial conditions\nneuron.set(neun_py.HHDoubleVariable.v, -80.0)  # Membrane potential\nneuron.set(neun_py.HHDoubleVariable.m, 0.1)    # Na activation\nneuron.set(neun_py.HHDoubleVariable.n, 0.7)    # K activation\nneuron.set(neun_py.HHDoubleVariable.h, 0.01)   # Na inactivation\n\n# Step 5: Run simulation\ndt = 0.001            # Time step (ms)\nT = 100               # Simulation duration (ms)\ntime = np.arange(0, T, dt)\n\nV = []  # Membrane potential storage\n\nfor t in time:\n    # Add external current input\n    neuron.add_synaptic_input(0.1)  # Constant input current\n    \n    # Step neuron forward\n    neuron.step(dt)\n    \n    # Record voltage\n    V.append(neuron.get(neun_py.HHDoubleVariable.v))\n\n# Step 6: Visualize\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.plot(time, V, 'b-', linewidth=1.5)\nax.set_xlabel('Time (ms)')\nax.set_ylabel('Membrane Potential (mV)')\nax.set_title('Hodgkin-Huxley Neuron - Basic Simulation')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Simulation completed: {len(time)} time steps\")\nprint(f\"Final voltage: {V[-1]:.2f} mV\")\n\n\n\n\n\n\n\nNoteUnderstanding the Workflow\n\n\n\nThe basic Neun workflow consists of:\n\nInstantiate neuron with model type, precision, and integrator\nCreate constructor args for the neuron model and set parameters using enums (e.g., HHDoubleParameter.cm)\n[Optional] Set initial conditions for variables\nSimulate by stepping through time with neuron.step(dt)\nAdd inputs with neuron.add_synaptic_input(current)\nRead state using neuron.get(variable_enum)",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Intro to Neun"
    ]
  },
  {
    "objectID": "02-01-neun-basics.html#key-takeaways",
    "href": "02-01-neun-basics.html#key-takeaways",
    "title": "2.1 Getting Started with Neun",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nNeun workflow: Create args ‚Üí Instantiate model ‚Üí Set params ‚Üí Set vars ‚Üí Simulate ‚Üí Record\nType safety: Use enums for parameters and variables (e.g., HHDoubleParameter.cm)\nModel naming: {Model}{Precision}{Integrator} (e.g., HHDoubleRK4)\nSynapses: Named as {Type}{Neuron1}{Neuron2}{Precision}{Integrator}\nOrganization: Write helper functions for creating and configuring neurons",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Intro to Neun"
    ]
  },
  {
    "objectID": "02-01-neun-basics.html#to-learn-more",
    "href": "02-01-neun-basics.html#to-learn-more",
    "title": "2.1 Getting Started with Neun",
    "section": "To learn more",
    "text": "To learn more\nIn this workshop we will use Neun in Python, but you can also program your simulations with Neun directly in C++. If you want to use Neun in C++ you can find examples in this repository: github.com/GNB-UAM/Neun-Example",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Intro to Neun"
    ]
  },
  {
    "objectID": "02-01-neun-basics.html#next-steps",
    "href": "02-01-neun-basics.html#next-steps",
    "title": "2.1 Getting Started with Neun",
    "section": "Next steps",
    "text": "Next steps\nNow that you‚Äôre familiar with Neun basics, we‚Äôll move on to:\n\nSimulating different neural models\nVisualizing your simulations\nTuning parameters\nAnalyzing neural behavior\n\n\nContinue to 2.2: Modeling Neurons",
    "crumbs": [
      "Part 2: Simulations with Neun",
      "Intro to Neun"
    ]
  },
  {
    "objectID": "01-intro-compneuro.html",
    "href": "01-intro-compneuro.html",
    "title": "Part 1: Introduction to Computational Neuroscience",
    "section": "",
    "text": "The brain is one of the most complex systems in nature. Understanding how neurons communicate, process information, and generate behavior requires tools that can bridge multiple scales‚Äîfrom molecular mechanisms to network dynamics and cognitive functions.\n\n\nConsider these facts:\n\nEach neuron can connect to thousands of other neurons\nNeural signals process information in multiple ways (rate-coding, temporal coding‚Ä¶) and on timescales from milliseconds to hours\nBrain functions emerge from the collective activity of neural populations\n\nComputational models help us:\n\nFormalize hypotheses about neural function\nPredict neural behavior under stimulation or different conditions\nCheck theories that would be difficult or impossible to test experimentally\nIntegrate findings across different scales and modalities\n\n\nThat enables a cycle of: Experimental measurement -&gt; Model building -&gt; Simulation -&gt; Prediction -&gt; Experimental validation.",
    "crumbs": [
      "Part 1: Computational Neuroscience"
    ]
  },
  {
    "objectID": "01-intro-compneuro.html#why-computational-neuroscience",
    "href": "01-intro-compneuro.html#why-computational-neuroscience",
    "title": "Part 1: Introduction to Computational Neuroscience",
    "section": "",
    "text": "The brain is one of the most complex systems in nature. Understanding how neurons communicate, process information, and generate behavior requires tools that can bridge multiple scales‚Äîfrom molecular mechanisms to network dynamics and cognitive functions.\n\n\nConsider these facts:\n\nEach neuron can connect to thousands of other neurons\nNeural signals process information in multiple ways (rate-coding, temporal coding‚Ä¶) and on timescales from milliseconds to hours\nBrain functions emerge from the collective activity of neural populations\n\nComputational models help us:\n\nFormalize hypotheses about neural function\nPredict neural behavior under stimulation or different conditions\nCheck theories that would be difficult or impossible to test experimentally\nIntegrate findings across different scales and modalities\n\n\nThat enables a cycle of: Experimental measurement -&gt; Model building -&gt; Simulation -&gt; Prediction -&gt; Experimental validation.",
    "crumbs": [
      "Part 1: Computational Neuroscience"
    ]
  },
  {
    "objectID": "01-intro-compneuro.html#levels-of-neural-modeling",
    "href": "01-intro-compneuro.html#levels-of-neural-modeling",
    "title": "Part 1: Introduction to Computational Neuroscience",
    "section": "Levels of Neural Modeling",
    "text": "Levels of Neural Modeling\nNeural systems can be modeled at different levels of abstraction:\n\n1. Biophysical Models\nDetail: High (ion channels, membrane properties)\nScale: Single neuron\nExamples: Hodgkin-Huxley model, multi-compartment models\nThese models capture detailed mechanisms but are computationally expensive.\n\n\n2. Simplified Neuron Models\nDetail: Medium (integrate-and-fire, adaptive threshold)\nScale: Single neuron to small networks\nExamples: Izhikevich model.\nBalance between biological realism and computational efficiency.\n\n\n3. Rate Models\nDetail: Low (average firing rates)\nScale: Large networks\nExamples: Wilson-Cowan model, neural field models\nCapture population dynamics efficiently.\n\n\n4. Abstract Models\nDetail: Minimal (information processing)\nScale: System-level\nExamples: Artificial neural networks, cognitive decision-making models\nFocus on function rather than biological detail. \n\n\n\n\n\n\nTipChoosing the Right Level\n\n\n\nThe appropriate level of modeling depends on your question:\n\nStudying ion channel responses? ‚Üí Biophysical model\nUnderstanding network interactions? ‚Üí Simplified neuron model\nModeling sensory processing across large populations? ‚Üí Rate model\nTesting cognitive theories? ‚Üí Abstract model",
    "crumbs": [
      "Part 1: Computational Neuroscience"
    ]
  },
  {
    "objectID": "01-intro-compneuro.html#neuron-dynamics",
    "href": "01-intro-compneuro.html#neuron-dynamics",
    "title": "Part 1: Introduction to Computational Neuroscience",
    "section": "Neuron Dynamics",
    "text": "Neuron Dynamics\nNeurons encode information by using electrical signals. Their membrane potential changes in response to internal states and external stimuli. As a result, neurons produce action potentials (spikes).\n\n\nVideo\nAction Potential Propagation\n\n\n*Video showing action potential propagation through a neuron. Source: Wikimedia Commons. By Laurentaylorj - Own work, CC BY-SA 3.0\n\nThe Neuron as an Electrical Circuit\nA neuron can be modelled as an electrical circuit with:\n\n\n\nCapacitance (C): Cell membrane stores charge\n\nResistance (R): Ion channels control current flow\n\nCurrent sources (I): Synaptic inputs and intrinsic currents\n\n\n\n\n\n\n\n\nInformation coding in Neurons\nActions potentials are the primary means of neural communication. Neurons can encode information using a spiking signal in several ways: - Shape of the action potentials (spikes): conveys information about the state of a neuron - Firing rate: Average number of spikes per unit time - Temporal coding: Precise timing of action potentials\n\nBut neurons do not operate in isolation.",
    "crumbs": [
      "Part 1: Computational Neuroscience"
    ]
  },
  {
    "objectID": "01-intro-compneuro.html#from-neurons-to-networks-synapses",
    "href": "01-intro-compneuro.html#from-neurons-to-networks-synapses",
    "title": "Part 1: Introduction to Computational Neuroscience",
    "section": "From Neurons to Networks: Synapses",
    "text": "From Neurons to Networks: Synapses\n\nWhy networks matter\nIndividual neurons are limited in their computational capabilities. Complex nervous functions emerge from networks of interconnected neurons. For instance, a simple learning circuit in a snail may contain around 20‚Äì50 neurons with roughly 1,000‚Äì2,000 synapses, a small cortical microcircuit in a mouse can involve 100 neurons forming 100,000‚Äì1,000,000 synapses, and human cortical circuits feature neurons with 10,000‚Äì30,000 synapses each, resulting in billions of connections. This dramatic increase in connectivity underlies the emergence of more sophisticated behaviors and cognitive functions.\n\n\nSynaptic connections\nNeurons communicate through synapses:\n\nChemical synapses: Release neurotransmitters (excitatory or inhibitory)\nElectrical synapses: Direct electrical coupling through gap junctions\n\nThe strength and dynamics of synaptic connections determine network behavior.\n\n\n\nTypes of synapses\n\n\n\n\n\nNetwork Architecture\nDifferent connection patterns lead to different dynamics:\n\nFeedforward networks: Information flows in one direction\nRecurrent networks: Neurons connect back to each other like Central Pattern Generators (CPGs)\nStructured networks: Specific patterns (e.g., columnar organization)\n\nRecurrent connections can produce robust and variable rhythmic dynamics.\n\n\nPopulation Dynamics\nIn large networks, we often care about population-level properties:\n\nFiring rates: Average number of spikes per unit time\nSynchrony: How coordinated is the activity?\nOscillations: Rhythmic patterns of activity\nFrequency responses: How does the network respond to inputs at different frequencies?",
    "crumbs": [
      "Part 1: Computational Neuroscience"
    ]
  },
  {
    "objectID": "01-intro-compneuro.html#key-principles-for-modeling",
    "href": "01-intro-compneuro.html#key-principles-for-modeling",
    "title": "Part 1: Introduction to Computational Neuroscience",
    "section": "Key Principles for Modeling",
    "text": "Key Principles for Modeling\nComputational models help interpret experimental findings:\n\nForward problem: Given a model, predict what measurements should look like\nInverse problem: Given measurements, infer underlying neural properties\nParameter estimation: Fit model parameters to match data\nHypothesis testing: Use models to generate testable predictions\n\n\n\n\n\n\n\nTipThe Modeling Cycle\n\n\n\n\nFormulate a hypothesis about neural function\nBuild a computational model\nSimulate and analyze model behavior\nCompare with experimental data\nRefine the model and repeat\n\n\n\n\n\n\n\n\n\n\n\nNoteKey Takeaways\n\n\n\n\nModels are tools: They help us understand, predict, and test ideas about neural systems\nChoose the right level: Match model complexity to your question\nIntegration matters: Models are most powerful when combined with experiments\nFrequency matters: Neural circuits have frequency-dependent properties that are crucial for function\nStart simple, build up: Begin with simple models and add complexity as needed",
    "crumbs": [
      "Part 1: Computational Neuroscience"
    ]
  },
  {
    "objectID": "01-intro-compneuro.html#looking-ahead",
    "href": "01-intro-compneuro.html#looking-ahead",
    "title": "Part 1: Introduction to Computational Neuroscience",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nNow that we understand why and what we model in computational neuroscience, we‚Äôre ready to learn how to implement these models using the Neun library.\nIn the next sections, we‚Äôll:\n\nSet up and explore the Neun library\nImplement single neuron models\nBuild neural networks\nSimulate complex dynamics\nAnalyze and visualize results\n\n\nüëâ Continue to Part 2: Getting Started with Neun",
    "crumbs": [
      "Part 1: Computational Neuroscience"
    ]
  },
  {
    "objectID": "01-intro-compneuro.html#further-reading",
    "href": "01-intro-compneuro.html#further-reading",
    "title": "Part 1: Introduction to Computational Neuroscience",
    "section": "Further Reading",
    "text": "Further Reading\n\nDayan, P., & Abbott, L. F. (2001). Theoretical Neuroscience. MIT Press.\nGerstner, W., Kistler, W. M., Naud, R., & Paninski, L. (2014). Neuronal Dynamics. Cambridge University Press.\nErmentrout, G. B., & Terman, D. H. (2010). Mathematical Foundations of Neuroscience. Springer.",
    "crumbs": [
      "Part 1: Computational Neuroscience"
    ]
  },
  {
    "objectID": "best-coding-practices.html",
    "href": "best-coding-practices.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "best-coding-practices.html#best-coding-practices",
    "href": "best-coding-practices.html#best-coding-practices",
    "title": "",
    "section": "Best Coding Practices",
    "text": "Best Coding Practices\n\n1. Organize Your Code\n# Good practice: organize simulation code\ndef create_hh_neuron():\n    \"\"\"Create a Hodgkin-Huxley neuron with standard parameters.\"\"\"\n    args = neun_py.HHDoubleConstructorArgs()\n    neuron = neun_py.HHDoubleRK4(args)\n    \n    # Set standard HH parameters\n    neuron.set_param(neun_py.HHDoubleParameter.cm, 1.0 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.vna, 50.0)\n    neuron.set_param(neun_py.HHDoubleParameter.vk, -77.0)\n    neuron.set_param(neun_py.HHDoubleParameter.vl, -54.387)\n    neuron.set_param(neun_py.HHDoubleParameter.gna, 120 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.gk, 36 * 7.854e-3)\n    neuron.set_param(neun_py.HHDoubleParameter.gl, 0.3 * 7.854e-3)\n    \n    # Set initial conditions\n    neuron.set(neun_py.HHDoubleVariable.v, -80.0)\n    neuron.set(neun_py.HHDoubleVariable.m, 0.1)\n    neuron.set(neun_py.HHDoubleVariable.n, 0.7)\n    neuron.set(neun_py.HHDoubleVariable.h, 0.01)\n    \n    return neuron\n\ndef run_simulation(neuron, I_ext, dt, T):\n    \"\"\"Run simulation and return time and voltage arrays.\"\"\"\n    time = np.arange(0, T, dt)\n    V = []\n    \n    for t in time:\n        neuron.add_synaptic_input(I_ext)\n        neuron.step(dt)\n        V.append(neuron.get(neun_py.HHDoubleVariable.v))\n    \n    return time, V\n\n# Use it\nneuron = create_hh_neuron()\ntime, V = run_simulation(neuron, I_ext=0.1, dt=0.001, T=100)\n\n\n2. Document Parameters\n# Always document what parameters mean and their units\nHH_PARAMS = {\n    'cm': 1.0 * 7.854e-3,      # Membrane capacitance (ŒºF/cm¬≤)\n    'vna': 50.0,               # Na reversal potential (mV)\n    'vk': -77.0,               # K reversal potential (mV)\n    'vl': -54.387,             # Leak reversal potential (mV)\n    'gna': 120 * 7.854e-3,     # Na conductance (mS/cm¬≤)\n    'gk': 36 * 7.854e-3,       # K conductance (mS/cm¬≤)\n    'gl': 0.3 * 7.854e-3       # Leak conductance (mS/cm¬≤)\n}\n\ndef set_hh_params(neuron, params):\n    \"\"\"Set HH neuron parameters from a dictionary.\"\"\"\n    neuron.set_param(neun_py.HHDoubleParameter.cm, params['cm'])\n    neuron.set_param(neun_py.HHDoubleParameter.vna, params['vna'])\n    neuron.set_param(neun_py.HHDoubleParameter.vk, params['vk'])\n    neuron.set_param(neun_py.HHDoubleParameter.vl, params['vl'])\n    neuron.set_param(neun_py.HHDoubleParameter.gna, params['gna'])\n    neuron.set_param(neun_py.HHDoubleParameter.gk, params['gk'])\n    neuron.set_param(neun_py.HHDoubleParameter.gl, params['gl'])\n\n\n3. Use Meaningful Variable Names\n# Good\nmembrane_voltage = neuron.get(neun_py.HHDoubleVariable.v)\nsodium_activation = neuron.get(neun_py.HHDoubleVariable.m)\n\n# Avoid\nv = neuron.get(neun_py.HHDoubleVariable.v)\nm = neuron.get(neun_py.HHDoubleVariable.m)\n\n\n4. Save Important Results\nimport json\n\n# Save results for later analysis\nresults = {\n    'model': 'HHDoubleRK4',\n    'parameters': HH_PARAMS,\n    'simulation': {\n        'dt': dt,\n        'T': T,\n        'external_current': 0.1\n    },\n    'voltage_trace': V,  # List is JSON serializable\n    'time': time.tolist()  # Convert numpy array to list\n}\n\nwith open('hh_simulation_results.json', 'w') as f:\n    json.dump(results, f, indent=2)"
  }
]